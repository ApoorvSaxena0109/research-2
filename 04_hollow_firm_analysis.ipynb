{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXwCRF8xtDt6"
   },
   "source": [
    "# The Hollow Firm: GenAI and Corporate Organizational Efficiency\n",
    "\n",
    "## Publication-Ready Analysis Notebook\n",
    "\n",
    "**Research Question:** Did generative AI (post-November 2022) enable high-exposure firms to \"hollow out\" their organizational structure, decoupling revenue growth from overhead costs?\n",
    "\n",
    "**Hypothesis:** High-AI-exposure firms reduced their SG&A-to-Revenue ratio (\"corporate bloat\") relative to low-exposure firms after ChatGPT's release.\n",
    "\n",
    "**Identification:** Difference-in-Differences with firm and time fixed effects, validated through randomization inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Structure\n",
    "1. Setup & Data Loading\n",
    "2. Variable Construction (SGA Efficiency, Winsorization)\n",
    "3. Main DiD Specification\n",
    "4. Randomization Inference (5,000 Permutations)\n",
    "5. Heterogeneity Analysis\n",
    "6. Advanced: Synthetic Control & Causal Forest\n",
    "7. Publication-Ready Tables & Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1gEigIe6tDt8",
    "outputId": "87022587-cfa7-4e74-8cb7-7361fd3efa8b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 0: ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d8pmXiWZtDt-",
    "outputId": "4f3baba5-fda8-4d9b-96d8-dc8c65aeb3cc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.1/581.1 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.2/607.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h✓ Packages installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install linearmodels pyfixest econml doubleml pysyncon joblib tqdm -q\n",
    "print(\"✓ Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rwpdsD9WtDt_",
    "outputId": "55a1e3fd-f54a-4b29-e6b4-4e1f81d1e190",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Libraries loaded\n",
      "✓ CPU cores available: 2\n",
      "✓ Permutation iterations: 5,000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Econometrics\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.panel import PanelOLS\n",
    "from scipy import stats\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Parallel processing (utilize Colab Pro cores)\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Advanced causal inference\n",
    "try:\n",
    "    from econml.dml import CausalForestDML\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    ECONML_AVAILABLE = True\n",
    "except:\n",
    "    ECONML_AVAILABLE = False\n",
    "    print(\"⚠ EconML not available - Causal Forest will be skipped\")\n",
    "\n",
    "# Configuration\n",
    "N_CORES = multiprocessing.cpu_count()\n",
    "N_PERMUTATIONS = 5000  # Randomization inference iterations\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plot style for publication\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Data path\n",
    "DATA_PATH = Path('/content/drive/MyDrive/Paper_2')\n",
    "\n",
    "print(f\"✓ Libraries loaded\")\n",
    "print(f\"✓ CPU cores available: {N_CORES}\")\n",
    "print(f\"✓ Permutation iterations: {N_PERMUTATIONS:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IUPNksftDuB"
   },
   "source": [
    "---\n",
    "## Section 1: Data Loading and Panel Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eNLLt1sPtDuB",
    "outputId": "1317a5ee-2963-4392-fe25-a22fa2f88a91",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data_1 shape: (10000, 235)\n",
      "Data_2 shape: (7388, 235)\n",
      "Common columns for merge: {'Total Current Debt - Compustat [Latest Quarter - 16] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter - 32] ($USDmm, Historical rate)', 'R&D Expense - Compustat [LTM - 24] ($USDmm, Historical rate)', 'Company Name', 'Total Assets - Compustat [Latest Quarter - 8] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM - 28] ($USDmm, Historical rate)', 'Return on Assets % - Compustat [LTM]', 'Total Current Assets - Compustat [Latest Quarter - 4] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM - 4] ($USDmm, Historical rate)', 'Acquisitions - Compustat [LTM - 16] ($USDmm, Historical rate)', 'Acquisitions - Compustat [LTM - 24] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM] ($USDmm, Historical rate)', 'Net Income - Capital IQ [LTM - 32] ($USDmm, Historical rate)', 'Total Assets - Compustat [Latest Quarter - 16] ($USDmm, Historical rate)', 'Acquisitions - Compustat [LTM - 32] ($USDmm, Historical rate)', 'Total Current Assets - Compustat [Latest Quarter - 28] ($USDmm, Historical rate)', 'Total Assets - Compustat [Latest Quarter - 4] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter - 24] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM - 32] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter - 8] ($USDmm, Historical rate)', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter - 4] ($USDmm, Historical rate)', 'Capital Expenditures - Compustat [LTM - 4] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter - 8] ($USDmm, Historical rate)', 'EBITDA - Capital IQ [LTM - 36] ($USDmm, Historical rate)', 'Net Income - Capital IQ [LTM - 24] ($USDmm, Historical rate)', 'Long-Term Debt - Compustat [Latest Quarter - 28] ($USDmm, Historical rate)', 'Operating Income - Capital IQ [LTM - 16] ($USDmm, Historical rate)', 'Total Current Assets - Compustat [Latest Quarter - 24] ($USDmm, Historical rate)', 'Total Current Debt - Compustat [Latest Quarter - 28] ($USDmm, Historical rate)', 'Total Current Debt - Compustat [Latest Quarter - 24] ($USDmm, Historical rate)', 'Return on Equity % - Compustat [LTM - 8]', 'Total Assets - Compustat [Latest Quarter] ($USDmm, Historical rate)', 'R&D Expense - Compustat [LTM - 8] ($USDmm, Historical rate)', 'Total Assets - Compustat [Latest Quarter - 36] ($USDmm, Historical rate)', 'Gross Profit - Capital IQ [LTM - 4] ($USDmm, Historical rate)', 'Gross Profit - Capital IQ [LTM - 16] ($USDmm, Historical rate)', 'Net Income - Capital IQ [LTM - 36] ($USDmm, Historical rate)', 'Total Employees - Compustat [Latest Quarter - 8]', 'Revenue - Compustat [LTM - 16] ($USDmm, Historical rate)', 'Long-Term Debt - Compustat [Latest Quarter - 36] ($USDmm, Historical rate)', 'Total Inventories - Compustat [Latest Quarter - 16] ($USDmm, Historical rate)', 'Total Current Assets - Compustat [Latest Quarter - 36] ($USDmm, Historical rate)', 'Total Current Debt - Compustat [Latest Quarter - 12] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter - 16] ($USDmm, Historical rate)', 'Total Employees - Compustat [Latest Quarter - 20]', 'Market Capitalization [My Setting] [Latest - 5 Year(s)] ($USDmm, Historical rate)', 'Total Debt/Equity - Compustat [LTM - 20]', 'Common Dividends - Compustat [LTM - 36] ($USD, Historical rate)', 'Long-Term Debt - Compustat [Latest Quarter - 32] ($USDmm, Historical rate)', 'Total Inventories - Compustat [Latest Quarter - 12] ($USDmm, Historical rate)', 'Long-Term Debt - Compustat [Latest Quarter - 24] ($USDmm, Historical rate)', 'Total Assets - Compustat [Latest Quarter - 32] ($USDmm, Historical rate)', 'Operating Income - Capital IQ [LTM - 24] ($USDmm, Historical rate)', 'Common Dividends - Compustat [LTM - 4] ($USD, Historical rate)', 'Return on Assets % - Compustat [LTM - 32]', 'Long-Term Debt - Compustat [Latest Quarter - 20] ($USDmm, Historical rate)', 'Total Debt/Equity - Compustat [LTM - 24]', 'Total Debt/Equity - Compustat [LTM - 16]', 'Total Assets - Compustat [Latest Quarter - 28] ($USDmm, Historical rate)', 'Market Capitalization [My Setting] [Latest - 8 Year(s)] ($USDmm, Historical rate)', 'EBITDA - Capital IQ [LTM - 20] ($USDmm, Historical rate)', 'Long-Term Debt - Compustat [Latest Quarter - 8] ($USDmm, Historical rate)', 'Net Income - Capital IQ [LTM - 12] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM - 4] ($USDmm, Historical rate)', 'Gross Profit - Capital IQ [LTM] ($USDmm, Historical rate)', 'Return on Assets % - Compustat [LTM - 8]', 'R&D Expense - Compustat [LTM] ($USDmm, Historical rate)', 'Total Inventories - Compustat [Latest Quarter - 32] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM - 16] ($USDmm, Historical rate)', 'Total Debt/Equity - Compustat [LTM - 36]', 'Return on Assets % - Compustat [LTM - 36]', 'Return on Assets % - Compustat [LTM - 24]', 'Total Inventories - Compustat [Latest Quarter - 24] ($USDmm, Historical rate)', 'EBITDA - Capital IQ [LTM - 8] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM - 8] ($USDmm, Historical rate)', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter - 24] ($USDmm, Historical rate)', 'Capital Expenditures - Compustat [LTM - 28] ($USDmm, Historical rate)', 'Total Employees - Compustat [Latest Quarter]', 'Return on Equity % - Compustat [LTM - 32]', 'Gross Profit - Capital IQ [LTM - 20] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM - 20] ($USDmm, Historical rate)', 'Total Employees - Compustat [Latest Quarter - 24]', 'EBITDA - Capital IQ [LTM - 4] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM - 36] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter - 32] ($USDmm, Historical rate)', 'Company Type', 'Net Income - Capital IQ [LTM - 16] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM] ($USDmm, Historical rate)', 'R&D Expense - Compustat [LTM - 16] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter - 12] ($USDmm, Historical rate)', 'Total Debt/Equity - Compustat [LTM - 28]', 'R&D Expense - Compustat [LTM - 32] ($USDmm, Historical rate)', 'Gross Profit - Capital IQ [LTM - 36] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter - 36] ($USDmm, Historical rate)', 'Market Capitalization [My Setting] [Latest - 6 Year(s)] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter - 16] ($USDmm, Historical rate)', 'Market Capitalization [My Setting] [Latest - 3 Year(s)] ($USDmm, Historical rate)', 'Total Debt/Equity - Compustat [LTM - 32]', 'Revenue - Compustat [LTM - 12] ($USDmm, Historical rate)', 'Return on Equity % - Compustat [LTM - 4]', 'EBITDA - Capital IQ [LTM] ($USDmm, Historical rate)', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter] ($USDmm, Historical rate)', 'Operating Income - Capital IQ [LTM - 32] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter - 36] ($USDmm, Historical rate)', 'Capital Expenditures - Compustat [LTM - 36] ($USDmm, Historical rate)', 'Return on Assets % - Compustat [LTM - 16]', 'Total Current Debt - Compustat [Latest Quarter - 8] ($USDmm, Historical rate)', 'Total Current Debt - Compustat [Latest Quarter] ($USDmm, Historical rate)', 'Total Current Debt - Compustat [Latest Quarter - 36] ($USDmm, Historical rate)', 'Total Current Assets - Compustat [Latest Quarter] ($USDmm, Historical rate)', 'Capital Expenditures - Compustat [LTM - 24] ($USDmm, Historical rate)', 'Total Debt/Equity - Compustat [LTM - 8]', 'EBITDA - Capital IQ [LTM - 12] ($USDmm, Historical rate)', 'R&D Expense - Compustat [LTM - 28] ($USDmm, Historical rate)', 'Common Dividends - Compustat [LTM - 20] ($USD, Historical rate)', 'Acquisitions - Compustat [LTM - 4] ($USDmm, Historical rate)', 'Gross Profit - Capital IQ [LTM - 24] ($USDmm, Historical rate)', 'Common Dividends - Compustat [LTM - 32] ($USD, Historical rate)', 'Total Debt/Equity - Compustat [LTM]', 'Capital Expenditures - Compustat [LTM - 12] ($USDmm, Historical rate)', 'Return on Equity % - Compustat [LTM - 36]', 'Total Inventories - Compustat [Latest Quarter - 28] ($USDmm, Historical rate)', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter - 28] ($USDmm, Historical rate)', 'Total Debt/Equity - Compustat [LTM - 12]', 'Total Employees - Compustat [Latest Quarter - 36]', 'Operating Income - Capital IQ [LTM] ($USDmm, Historical rate)', 'Return on Assets % - Compustat [LTM - 20]', 'Gross Profit - Capital IQ [LTM - 28] ($USDmm, Historical rate)', 'Common Dividends - Compustat [LTM] ($USD, Historical rate)', 'EBITDA - Capital IQ [LTM - 24] ($USDmm, Historical rate)', 'Total Current Debt - Compustat [Latest Quarter - 32] ($USDmm, Historical rate)', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter - 32] ($USDmm, Historical rate)', 'EBITDA - Capital IQ [LTM - 16] ($USDmm, Historical rate)', 'Common Dividends - Compustat [LTM - 24] ($USD, Historical rate)', 'Net Income - Capital IQ [LTM - 20] ($USDmm, Historical rate)', 'Common Dividends - Compustat [LTM - 8] ($USD, Historical rate)', 'Net Income - Capital IQ [LTM - 8] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter - 4] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter - 24] ($USDmm, Historical rate)', 'Return on Equity % - Compustat [LTM - 24]', 'Market Capitalization [My Setting] [Latest] ($USDmm, Historical rate)', 'Common Dividends - Compustat [LTM - 28] ($USD, Historical rate)', 'Gross Profit - Capital IQ [LTM - 32] ($USDmm, Historical rate)', 'Market Capitalization [My Setting] [Latest - 2 Year(s)] ($USDmm, Historical rate)', 'Operating Income - Capital IQ [LTM - 12] ($USDmm, Historical rate)', 'Total Employees - Compustat [Latest Quarter - 12]', 'Operating Income - Capital IQ [LTM - 36] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter - 12] ($USDmm, Historical rate)', 'Capital Expenditures - Compustat [LTM - 20] ($USDmm, Historical rate)', 'Common Dividends - Compustat [LTM - 12] ($USD, Historical rate)', 'Total Debt/Equity - Compustat [LTM - 4]', 'Operating Income - Capital IQ [LTM - 20] ($USDmm, Historical rate)', 'Long-Term Debt - Compustat [Latest Quarter - 12] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM - 24] ($USDmm, Historical rate)', 'Total Assets - Compustat [Latest Quarter - 20] ($USDmm, Historical rate)', 'Acquisitions - Compustat [LTM - 12] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter - 4] ($USDmm, Historical rate)', 'Return on Assets % - Compustat [LTM - 28]', 'Capital Expenditures - Compustat [LTM - 16] ($USDmm, Historical rate)', 'Total Inventories - Compustat [Latest Quarter - 4] ($USDmm, Historical rate)', 'Total Current Assets - Compustat [Latest Quarter - 12] ($USDmm, Historical rate)', 'Total Current Assets - Compustat [Latest Quarter - 8] ($USDmm, Historical rate)', 'Total Employees - Compustat [Latest Quarter - 32]', 'R&D Expense - Compustat [LTM - 20] ($USDmm, Historical rate)', 'Net Income - Capital IQ [LTM - 4] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM - 36] ($USDmm, Historical rate)', 'Return on Equity % - Compustat [LTM - 16]', 'Total Employees - Compustat [Latest Quarter - 4]', 'Total Employees - Compustat [Latest Quarter - 16]', 'Total Current Debt - Compustat [Latest Quarter - 4] ($USDmm, Historical rate)', 'Long-Term Debt - Compustat [Latest Quarter] ($USDmm, Historical rate)', 'Geographic Locations', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter - 16] ($USDmm, Historical rate)', 'Net Income - Capital IQ [LTM] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM - 12] ($USDmm, Historical rate)', 'Total Assets - Compustat [Latest Quarter - 12] ($USDmm, Historical rate)', 'Total Inventories - Compustat [Latest Quarter - 8] ($USDmm, Historical rate)', 'Market Capitalization [My Setting] [Latest - 9 Year(s)] ($USDmm, Historical rate)', 'Total Current Assets - Compustat [Latest Quarter - 32] ($USDmm, Historical rate)', 'Total Current Debt - Compustat [Latest Quarter - 20] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM - 20] ($USDmm, Historical rate)', 'Return on Equity % - Compustat [LTM - 20]', 'Return on Assets % - Compustat [LTM - 4]', 'Acquisitions - Compustat [LTM - 36] ($USDmm, Historical rate)', 'Total Inventories - Compustat [Latest Quarter] ($USDmm, Historical rate)', 'Return on Equity % - Compustat [LTM - 28]', 'Total Current Assets - Compustat [Latest Quarter - 16] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM - 24] ($USDmm, Historical rate)', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter - 20] ($USDmm, Historical rate)', 'Return on Equity % - Compustat [LTM]', 'Exchange:Ticker', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter - 8] ($USDmm, Historical rate)', 'Net Income - Capital IQ [LTM - 28] ($USDmm, Historical rate)', 'Market Capitalization [My Setting] [Latest - 4 Year(s)] ($USDmm, Historical rate)', 'Acquisitions - Compustat [LTM - 8] ($USDmm, Historical rate)', 'Total Employees - Compustat [Latest Quarter - 28]', 'Industry Classifications', 'Capital Expenditures - Compustat [LTM] ($USDmm, Historical rate)', 'Market Capitalization [My Setting] [Latest - 1 Year(s)] ($USDmm, Historical rate)', 'EBITDA - Capital IQ [LTM - 32] ($USDmm, Historical rate)', 'Gross Profit - Capital IQ [LTM - 12] ($USDmm, Historical rate)', 'Interest Expense - Capital IQ [LTM - 28] ($USDmm, Historical rate)', 'Acquisitions - Compustat [LTM] ($USDmm, Historical rate)', 'Operating Income - Capital IQ [LTM - 28] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM - 8] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter - 20] ($USDmm, Historical rate)', 'Total Inventories - Compustat [Latest Quarter - 36] ($USDmm, Historical rate)', 'Total Inventories - Compustat [Latest Quarter - 20] ($USDmm, Historical rate)', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter - 36] ($USDmm, Historical rate)', 'Gross Property, Plant & Equipment - Compustat [Latest Quarter - 12] ($USDmm, Historical rate)', 'EBITDA - Capital IQ [LTM - 28] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter - 20] ($USDmm, Historical rate)', 'Long-Term Debt - Compustat [Latest Quarter - 16] ($USDmm, Historical rate)', 'Gross Profit - Capital IQ [LTM - 8] ($USDmm, Historical rate)', 'Common Dividends - Compustat [LTM - 16] ($USD, Historical rate)', 'Market Capitalization [My Setting] [Latest - 7 Year(s)] ($USDmm, Historical rate)', 'R&D Expense - Compustat [LTM - 12] ($USDmm, Historical rate)', 'Acquisitions - Compustat [LTM - 28] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM - 32] ($USDmm, Historical rate)', 'R&D Expense - Compustat [LTM - 4] ($USDmm, Historical rate)', 'Total Assets - Compustat [Latest Quarter - 24] ($USDmm, Historical rate)', 'Return on Equity % - Compustat [LTM - 12]', 'Total Current Assets - Compustat [Latest Quarter - 20] ($USDmm, Historical rate)', 'Total Receivables - Compustat [Latest Quarter - 28] ($USDmm, Historical rate)', 'Operating Income - Capital IQ [LTM - 4] ($USDmm, Historical rate)', 'Operating Income - Capital IQ [LTM - 8] ($USDmm, Historical rate)', 'Cash & Short-Term Investments - Compustat [Latest Quarter - 28] ($USDmm, Historical rate)', 'Return on Assets % - Compustat [LTM - 12]', 'Acquisitions - Compustat [LTM - 20] ($USDmm, Historical rate)', 'R&D Expense - Compustat [LTM - 36] ($USDmm, Historical rate)', 'Long-Term Debt - Compustat [Latest Quarter - 4] ($USDmm, Historical rate)', 'Capital Expenditures - Compustat [LTM - 8] ($USDmm, Historical rate)', 'Capital Expenditures - Compustat [LTM - 32] ($USDmm, Historical rate)'}\n",
      "Combined shape: (17388, 235)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_combine_data(data_path):\n",
    "    \"\"\"\n",
    "    Load and combine the two Excel files.\n",
    "    Returns a single DataFrame.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_excel(data_path / 'Data_1.xlsx')\n",
    "    df2 = pd.read_excel(data_path / 'Data_2.xlsx')\n",
    "\n",
    "    print(f\"Data_1 shape: {df1.shape}\")\n",
    "    print(f\"Data_2 shape: {df2.shape}\")\n",
    "\n",
    "    # Check for common columns to merge on\n",
    "    common_cols = set(df1.columns) & set(df2.columns)\n",
    "\n",
    "    if common_cols:\n",
    "        print(f\"Common columns for merge: {common_cols}\")\n",
    "        df = pd.merge(df1, df2, on=list(common_cols), how='outer')\n",
    "    elif len(df1) == len(df2):\n",
    "        print(\"Same row count - concatenating horizontally\")\n",
    "        df = pd.concat([df1, df2], axis=1)\n",
    "    else:\n",
    "        print(\"Using Data_1 as primary\")\n",
    "        df = df1\n",
    "\n",
    "    print(f\"Combined shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df_wide = load_and_combine_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Q_7d3iyTtDuC",
    "outputId": "c5d99f11-3969-46e0-9f0f-67692959aef4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Identified Columns:\n",
      "============================================================\n",
      "firm_id: Exchange:Ticker\n",
      "firm_name: Company Name\n",
      "industry: Industry Classifications\n",
      "revenue: 10 columns found\n",
      "   Sample: ['Revenue - Compustat [LTM - 36] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM - 32] ($USDmm, Historical rate)', 'Revenue - Compustat [LTM - 28] ($USDmm, Historical rate)']\n",
      "sga: 0 columns found\n",
      "employees: 10 columns found\n",
      "   Sample: ['Total Employees - Compustat [Latest Quarter - 36]', 'Total Employees - Compustat [Latest Quarter - 32]', 'Total Employees - Compustat [Latest Quarter - 28]']\n",
      "ebitda: 10 columns found\n",
      "   Sample: ['EBITDA - Capital IQ [LTM - 36] ($USDmm, Historical rate)', 'EBITDA - Capital IQ [LTM - 32] ($USDmm, Historical rate)', 'EBITDA - Capital IQ [LTM - 28] ($USDmm, Historical rate)']\n",
      "total_assets: 10 columns found\n",
      "   Sample: ['Total Assets - Compustat [Latest Quarter - 36] ($USDmm, Historical rate)', 'Total Assets - Compustat [Latest Quarter - 32] ($USDmm, Historical rate)', 'Total Assets - Compustat [Latest Quarter - 28] ($USDmm, Historical rate)']\n",
      "intangibles: 0 columns found\n",
      "market_cap: 10 columns found\n",
      "   Sample: ['Market Capitalization [My Setting] [Latest - 9 Year(s)] ($USDmm, Historical rate)', 'Market Capitalization [My Setting] [Latest - 8 Year(s)] ($USDmm, Historical rate)', 'Market Capitalization [My Setting] [Latest - 7 Year(s)] ($USDmm, Historical rate)']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COLUMN IDENTIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def identify_columns(df):\n",
    "    \"\"\"\n",
    "    Automatically identify key columns in the dataset.\n",
    "    \"\"\"\n",
    "    columns = {\n",
    "        'firm_id': None,\n",
    "        'firm_name': None,\n",
    "        'industry': None,\n",
    "        'revenue': [],\n",
    "        'sga': [],\n",
    "        'employees': [],\n",
    "        'ebitda': [],\n",
    "        'total_assets': [],\n",
    "        'intangibles': [],\n",
    "        'market_cap': []\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "\n",
    "        # Identifiers\n",
    "        if 'ticker' in col_lower or 'symbol' in col_lower:\n",
    "            columns['firm_id'] = col\n",
    "        elif 'company' in col_lower and 'name' in col_lower:\n",
    "            columns['firm_name'] = col\n",
    "        elif 'industry' in col_lower or 'sector' in col_lower:\n",
    "            columns['industry'] = col\n",
    "\n",
    "        # Financial metrics (time series)\n",
    "        elif 'revenue' in col_lower or 'sales' in col_lower:\n",
    "            columns['revenue'].append(col)\n",
    "        elif 'sg&a' in col_lower or 'sga' in col_lower or 'selling' in col_lower:\n",
    "            columns['sga'].append(col)\n",
    "        elif 'employee' in col_lower:\n",
    "            columns['employees'].append(col)\n",
    "        elif 'ebitda' in col_lower:\n",
    "            columns['ebitda'].append(col)\n",
    "        elif 'total asset' in col_lower:\n",
    "            columns['total_assets'].append(col)\n",
    "        elif 'intangible' in col_lower:\n",
    "            columns['intangibles'].append(col)\n",
    "        elif 'market cap' in col_lower:\n",
    "            columns['market_cap'].append(col)\n",
    "\n",
    "    return columns\n",
    "\n",
    "col_map = identify_columns(df_wide)\n",
    "\n",
    "print(\"\\nIdentified Columns:\")\n",
    "print(\"=\" * 60)\n",
    "for key, val in col_map.items():\n",
    "    if isinstance(val, list):\n",
    "        print(f\"{key}: {len(val)} columns found\")\n",
    "        if val:\n",
    "            print(f\"   Sample: {val[:3]}\")\n",
    "    else:\n",
    "        print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjGHJwkbtDuC"
   },
   "outputs": [],
   "source": "# ============================================================================\n# WIDE TO LONG PANEL CONVERSION\n# ============================================================================\n# FIXED: 'period' is now NUMERIC for PanelOLS compatibility\n\nimport re\n\ndef parse_time_from_column(col_name):\n    \"\"\"\n    Extract metric name and time offset from column name.\n\n    Returns: (metric_name, time_offset, time_type)\n    \"\"\"\n    patterns = [\n        (r'(.+?)\\s*\\[LTM(?:\\s*-\\s*(\\d+))?\\]', 'LTM'),\n        (r'(.+?)\\s*\\[Latest\\s*Quarter(?:\\s*-\\s*(\\d+))?\\]', 'Quarterly'),\n        (r'(.+?)\\s*\\[Latest(?:\\s*-\\s*(\\d+)\\s*Year)?', 'Annual'),\n    ]\n\n    for pattern, time_type in patterns:\n        match = re.search(pattern, col_name, re.IGNORECASE)\n        if match:\n            metric = match.group(1).strip()\n            offset = int(match.group(2)) if match.group(2) else 0\n            return (metric, offset, time_type)\n\n    return (col_name, None, None)\n\n\ndef create_long_panel(df_wide, firm_id_col, time_type='LTM',\n                      base_year=2024, base_quarter=4):\n    \"\"\"\n    Convert wide-format data to long panel format.\n    \n    FIXED: 'period' is now numeric (year*4+quarter) for PanelOLS compatibility.\n    String version is available as 'period_str'.\n    \"\"\"\n    # Parse all columns\n    parsed_cols = []\n    for col in df_wide.columns:\n        metric, offset, ttype = parse_time_from_column(col)\n        parsed_cols.append({\n            'original': col,\n            'metric': metric,\n            'offset': offset,\n            'type': ttype\n        })\n\n    col_df = pd.DataFrame(parsed_cols)\n    time_cols = col_df[col_df['type'] == time_type]\n\n    print(f\"Found {len(time_cols)} {time_type} columns\")\n\n    # Get time offsets\n    offsets = sorted(time_cols['offset'].dropna().unique())\n    print(f\"Time offsets: {offsets}\")\n\n    # Build panel\n    panels = []\n\n    for offset in offsets:\n        period_cols = time_cols[time_cols['offset'] == offset]\n        col_mapping = dict(zip(period_cols['original'], period_cols['metric']))\n\n        # Select available columns\n        cols_to_use = [firm_id_col] + [c for c in col_mapping.keys() if c in df_wide.columns]\n\n        if len(cols_to_use) <= 1:\n            continue\n\n        period_df = df_wide[cols_to_use].copy()\n        period_df = period_df.rename(columns=col_mapping)\n        period_df['time_offset'] = offset\n\n        # Convert offset to calendar time\n        total_q = base_year * 4 + base_quarter - offset\n        period_df['year'] = (total_q - 1) // 4\n        period_df['quarter'] = ((total_q - 1) % 4) + 1\n\n        panels.append(period_df)\n\n    if not panels:\n        raise ValueError(f\"No panels created for time_type={time_type}\")\n\n    panel = pd.concat(panels, ignore_index=True)\n    panel = panel.sort_values([firm_id_col, 'time_offset']).reset_index(drop=True)\n\n    # FIXED: Create NUMERIC period identifier for PanelOLS\n    # yearquarter = year * 4 + quarter (e.g., 2022Q4 = 2022*4+4 = 8092)\n    panel['period'] = panel['year'] * 4 + panel['quarter']\n    panel['yearquarter'] = panel['period']  # Alias for compatibility\n    \n    # Create string version for display/labeling\n    panel['period_str'] = panel['year'].astype(str) + 'Q' + panel['quarter'].astype(str)\n    \n    print(f\"✓ Period column is NUMERIC (type: {panel['period'].dtype})\")\n\n    return panel"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wKcwzTmktDuD",
    "outputId": "b5862b75-6958-4016-9d7c-ba64493c9b06",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 130 LTM columns\n",
      "Time offsets: [np.float64(0.0), np.float64(4.0), np.float64(8.0), np.float64(12.0), np.float64(16.0), np.float64(20.0), np.float64(24.0), np.float64(28.0), np.float64(32.0), np.float64(36.0)]\n",
      "\n",
      "Panel created: (173880, 19)\n",
      "Firms: 7,736\n",
      "Periods: 10\n",
      "Time range: 2015.0-Q4.0 to 2024.0-Q4.0\n"
     ]
    }
   ],
   "source": [
    "# Create firm ID if not found\n",
    "FIRM_ID = col_map['firm_id']\n",
    "if FIRM_ID is None:\n",
    "    df_wide['firm_id'] = range(len(df_wide))\n",
    "    FIRM_ID = 'firm_id'\n",
    "    print(\"Created numeric firm_id\")\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATE THESE VALUES BASED ON YOUR DATA\n",
    "# ============================================================================\n",
    "BASE_YEAR = 2024      # What year does \"Latest\" refer to?\n",
    "BASE_QUARTER = 4      # What quarter? (1-4)\n",
    "\n",
    "# Create panel\n",
    "panel = create_long_panel(df_wide, FIRM_ID, time_type='LTM',\n",
    "                          base_year=BASE_YEAR, base_quarter=BASE_QUARTER)\n",
    "\n",
    "print(f\"\\nPanel created: {panel.shape}\")\n",
    "print(f\"Firms: {panel[FIRM_ID].nunique():,}\")\n",
    "print(f\"Periods: {panel['period'].nunique()}\")\n",
    "print(f\"Time range: {panel['year'].min()}-Q{panel[panel['year']==panel['year'].min()]['quarter'].min()} to {panel['year'].max()}-Q{panel[panel['year']==panel['year'].max()]['quarter'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sTA5aN8EtDuE",
    "outputId": "7b9c9f9b-5791-4743-b5af-8246ea059973",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Panel columns:\n",
      "['Exchange:Ticker', 'Gross Profit - Capital IQ', 'Operating Income - Capital IQ', 'EBITDA - Capital IQ', 'Net Income - Capital IQ', 'R&D Expense - Compustat', 'Interest Expense - Capital IQ', 'Common Dividends - Compustat', 'Capital Expenditures - Compustat', 'Acquisitions - Compustat', 'Return on Assets % - Compustat', 'Return on Equity % - Compustat', 'Total Debt/Equity - Compustat', 'Revenue - Compustat', 'time_offset', 'year', 'quarter', 'period', 'yearquarter']\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  Exchange:Ticker Gross Profit - Capital IQ Operating Income - Capital IQ  \\\n",
       "0               -                         -                             -   \n",
       "1               -                         -                             -   \n",
       "2               -                         -                             -   \n",
       "3               -                         -                             -   \n",
       "4               -                         -                             -   \n",
       "5               -                         -                             -   \n",
       "6               -                         -                             -   \n",
       "7               -                         -                             -   \n",
       "8               -                         -                             -   \n",
       "9               -                         -                             -   \n",
       "\n",
       "  EBITDA - Capital IQ Net Income - Capital IQ R&D Expense - Compustat  \\\n",
       "0                   -                       -                       -   \n",
       "1                   -                       -                       -   \n",
       "2                   -                       -                       -   \n",
       "3                   -                       -                       0   \n",
       "4                   -                       -                       -   \n",
       "5                   -                       -                       -   \n",
       "6                   -                       -                       -   \n",
       "7                   -                       -                       -   \n",
       "8                   -                       -                       -   \n",
       "9                   -                       -                       -   \n",
       "\n",
       "  Interest Expense - Capital IQ Common Dividends - Compustat  \\\n",
       "0                             -                            -   \n",
       "1                             -                            -   \n",
       "2                             -                            -   \n",
       "3                             -                            0   \n",
       "4                             -                            -   \n",
       "5                             -                            -   \n",
       "6                             -                            -   \n",
       "7                             -                            -   \n",
       "8                             -                            -   \n",
       "9                             -                            -   \n",
       "\n",
       "  Capital Expenditures - Compustat Acquisitions - Compustat  \\\n",
       "0                                -                        -   \n",
       "1                                -                        -   \n",
       "2                                -                        -   \n",
       "3                                -                        -   \n",
       "4                                -                        -   \n",
       "5                                -                        -   \n",
       "6                                -                        -   \n",
       "7                                -                        -   \n",
       "8                                -                        -   \n",
       "9                                -                        -   \n",
       "\n",
       "  Return on Assets % - Compustat Return on Equity % - Compustat  \\\n",
       "0                              -                              -   \n",
       "1                              -                              -   \n",
       "2                              -                              -   \n",
       "3                        10.3000                        36.9000   \n",
       "4                              -                              -   \n",
       "5                              -                              -   \n",
       "6                              -                              -   \n",
       "7                              -                              -   \n",
       "8                              -                              -   \n",
       "9                              -                              -   \n",
       "\n",
       "  Total Debt/Equity - Compustat Revenue - Compustat  time_offset      year  \\\n",
       "0                             -                   -       0.0000 2024.0000   \n",
       "1                             -                   -       0.0000 2024.0000   \n",
       "2                             -                   -       0.0000 2024.0000   \n",
       "3                             -             29.2000       0.0000 2024.0000   \n",
       "4                             -                   -       0.0000 2024.0000   \n",
       "5                             -                   -       0.0000 2024.0000   \n",
       "6                             -                   -       0.0000 2024.0000   \n",
       "7                             -                   -       0.0000 2024.0000   \n",
       "8                             -                   -       0.0000 2024.0000   \n",
       "9                             -                   -       0.0000 2024.0000   \n",
       "\n",
       "   quarter      period  yearquarter  \n",
       "0   4.0000  2024.0Q4.0    8100.0000  \n",
       "1   4.0000  2024.0Q4.0    8100.0000  \n",
       "2   4.0000  2024.0Q4.0    8100.0000  \n",
       "3   4.0000  2024.0Q4.0    8100.0000  \n",
       "4   4.0000  2024.0Q4.0    8100.0000  \n",
       "5   4.0000  2024.0Q4.0    8100.0000  \n",
       "6   4.0000  2024.0Q4.0    8100.0000  \n",
       "7   4.0000  2024.0Q4.0    8100.0000  \n",
       "8   4.0000  2024.0Q4.0    8100.0000  \n",
       "9   4.0000  2024.0Q4.0    8100.0000  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-9b8481c9-36a2-4b40-a53a-747487d4860b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exchange:Ticker</th>\n",
       "      <th>Gross Profit - Capital IQ</th>\n",
       "      <th>Operating Income - Capital IQ</th>\n",
       "      <th>EBITDA - Capital IQ</th>\n",
       "      <th>Net Income - Capital IQ</th>\n",
       "      <th>R&amp;D Expense - Compustat</th>\n",
       "      <th>Interest Expense - Capital IQ</th>\n",
       "      <th>Common Dividends - Compustat</th>\n",
       "      <th>Capital Expenditures - Compustat</th>\n",
       "      <th>Acquisitions - Compustat</th>\n",
       "      <th>Return on Assets % - Compustat</th>\n",
       "      <th>Return on Equity % - Compustat</th>\n",
       "      <th>Total Debt/Equity - Compustat</th>\n",
       "      <th>Revenue - Compustat</th>\n",
       "      <th>time_offset</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>period</th>\n",
       "      <th>yearquarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>10.3000</td>\n",
       "      <td>36.9000</td>\n",
       "      <td>-</td>\n",
       "      <td>29.2000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2024.0Q4.0</td>\n",
       "      <td>8100.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b8481c9-36a2-4b40-a53a-747487d4860b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9b8481c9-36a2-4b40-a53a-747487d4860b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9b8481c9-36a2-4b40-a53a-747487d4860b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-f0e5d5ba-9520-459c-a803-d531b479b925\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0e5d5ba-9520-459c-a803-d531b479b925')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-f0e5d5ba-9520-459c-a803-d531b479b925 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "summary": "{\n  \"name\": \"display(panel\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Exchange:Ticker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gross Profit - Capital IQ\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Operating Income - Capital IQ\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EBITDA - Capital IQ\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Net Income - Capital IQ\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R&D Expense - Compustat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Interest Expense - Capital IQ\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Common Dividends - Compustat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Capital Expenditures - Compustat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Acquisitions - Compustat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Return on Assets % - Compustat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          10.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Return on Equity % - Compustat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          36.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Debt/Equity - Compustat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Revenue - Compustat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          29.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_offset\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 2024.0,\n        \"max\": 2024.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2024.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quarter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 4.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"period\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2024.0Q4.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"yearquarter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 8100.0,\n        \"max\": 8100.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          8100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Preview panel\n",
    "print(\"\\nPanel columns:\")\n",
    "print(panel.columns.tolist())\n",
    "print(\"\\nSample rows:\")\n",
    "display(panel.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHDG5H_JtDuF"
   },
   "source": [
    "---\n",
    "## Section 2: Variable Construction\n",
    "\n",
    "**Key Variable:** SGA Efficiency = SG&A / Revenue\n",
    "\n",
    "This measures \"corporate bloat\" - how much overhead is required per dollar of revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2Z3acQutDuF",
    "outputId": "895bfcd8-a817-429b-e91a-6436dbfb1f84",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    }
   },
   "outputs": [],
   "source": "# ============================================================================\n# SECTION 2: VARIABLE CONSTRUCTION\n# ============================================================================\n# FIXED: Now computes SG&A proxy when direct SG&A column is not available\n# Uses Operating Expenses = Gross Profit - Operating Income\n#\n# NOTE: Employee data is MISSING in this dataset (all placeholders).\n# We use alternative productivity measures instead.\n\ndef winsorize(series, lower=0.01, upper=0.99):\n    \"\"\"\n    Winsorize a series at specified percentiles.\n    Standard practice: 1st and 99th percentiles.\n    \"\"\"\n    lower_bound = series.quantile(lower)\n    upper_bound = series.quantile(upper)\n    return series.clip(lower=lower_bound, upper=upper_bound)\n\n\ndef construct_variables(panel):\n    \"\"\"\n    Construct all analysis variables with proper handling.\n    \n    FIXED: Now computes SG&A proxy from Gross Profit - Operating Income\n    when direct SG&A column is not available.\n    \n    NOTE: Employee data is missing - uses alternative productivity measures.\n    \"\"\"\n    df = panel.copy()\n\n    # Identify all relevant financial columns\n    revenue_col = None\n    sga_col = None\n    employee_col = None\n    ebitda_col = None\n    assets_col = None\n    gross_profit_col = None\n    operating_income_col = None\n    rd_col = None\n    capex_col = None\n\n    for col in df.columns:\n        col_lower = col.lower()\n        if ('revenue' in col_lower or 'sales' in col_lower) and revenue_col is None:\n            revenue_col = col\n        elif ('sg&a' in col_lower or 'sga' in col_lower or 'selling' in col_lower) and sga_col is None:\n            sga_col = col\n        elif 'employee' in col_lower and employee_col is None:\n            employee_col = col\n        elif 'ebitda' in col_lower and ebitda_col is None:\n            ebitda_col = col\n        elif 'total asset' in col_lower and assets_col is None:\n            assets_col = col\n        elif 'gross profit' in col_lower and gross_profit_col is None:\n            gross_profit_col = col\n        elif 'operating income' in col_lower and operating_income_col is None:\n            operating_income_col = col\n        elif ('r&d' in col_lower or 'research' in col_lower) and rd_col is None:\n            rd_col = col\n        elif ('capex' in col_lower or 'capital expenditure' in col_lower) and capex_col is None:\n            capex_col = col\n\n    print(\"=\" * 60)\n    print(\"IDENTIFIED COLUMNS:\")\n    print(\"=\" * 60)\n    print(f\"Revenue: {revenue_col}\")\n    print(f\"SG&A: {sga_col}\")\n    print(f\"Gross Profit: {gross_profit_col}\")\n    print(f\"Operating Income: {operating_income_col}\")\n    print(f\"R&D: {rd_col}\")\n    print(f\"CapEx: {capex_col}\")\n    print(f\"Employee: {employee_col}\")\n    print(f\"EBITDA: {ebitda_col}\")\n    print(f\"Assets: {assets_col}\")\n\n    # ========================================================================\n    # CHECK EMPLOYEE DATA AVAILABILITY\n    # ========================================================================\n    employee_data_available = False\n    if employee_col:\n        # Convert to numeric if needed\n        if df[employee_col].dtype == 'object':\n            df[employee_col] = pd.to_numeric(\n                df[employee_col].replace(['-', 'NM', 'NA', 'N/A', '', ' '], np.nan), \n                errors='coerce'\n            )\n        \n        non_null_employees = df[employee_col].notna().sum()\n        if non_null_employees > 0:\n            employee_data_available = True\n            print(f\"\\n✓ Employee data available: {non_null_employees:,} observations\")\n        else:\n            print(f\"\\n⚠ WARNING: Employee data is COMPLETELY MISSING!\")\n            print(\"  All values are placeholders. Revenue-per-employee cannot be computed.\")\n            print(\"  Consider sourcing employee data from Compustat or annual reports.\")\n\n    # ========================================================================\n    # KEY VARIABLE: SGA EFFICIENCY (Corporate Bloat Measure)\n    # ========================================================================\n    \n    if revenue_col and sga_col:\n        # Direct SG&A available\n        mask = (df[revenue_col] > 0) & (df[sga_col].notna())\n        df['sga_efficiency_raw'] = np.nan\n        df.loc[mask, 'sga_efficiency_raw'] = df.loc[mask, sga_col] / df.loc[mask, revenue_col]\n        df['sga_efficiency'] = winsorize(df['sga_efficiency_raw'], 0.01, 0.99)\n        print(f\"\\n✓ SGA Efficiency from direct SG&A: {df['sga_efficiency'].notna().sum():,} obs\")\n        \n    elif revenue_col and gross_profit_col and operating_income_col:\n        # COMPUTE SG&A PROXY: Operating Expenses / Revenue\n        print(\"\\n⚠ SG&A column not found - computing proxy from Gross Profit - Operating Income\")\n        \n        mask = (df[revenue_col] > 0) & (df[gross_profit_col].notna()) & (df[operating_income_col].notna())\n        \n        # Operating Expenses = Gross Profit - Operating Income\n        df['operating_expenses'] = df[gross_profit_col] - df[operating_income_col]\n        \n        # Overhead Ratio = Operating Expenses / Revenue (proxy for SG&A efficiency)\n        df['sga_efficiency_raw'] = np.nan\n        df.loc[mask, 'sga_efficiency_raw'] = df.loc[mask, 'operating_expenses'] / df.loc[mask, revenue_col]\n        \n        # Only keep positive values (operating expenses should be positive)\n        df.loc[df['sga_efficiency_raw'] < 0, 'sga_efficiency_raw'] = np.nan\n        \n        # Winsorize\n        df['sga_efficiency'] = winsorize(df['sga_efficiency_raw'], 0.01, 0.99)\n        \n        print(f\"✓ SGA Efficiency (proxy) constructed: {df['sga_efficiency'].notna().sum():,} obs\")\n        if df['sga_efficiency'].notna().sum() > 0:\n            print(f\"  Mean: {df['sga_efficiency'].mean():.4f}\")\n            print(f\"  Median: {df['sga_efficiency'].median():.4f}\")\n        \n    else:\n        print(\"\\n✗ Cannot construct SGA Efficiency - missing required columns\")\n        df['sga_efficiency'] = np.nan\n\n    # ========================================================================\n    # ALTERNATIVE PRODUCTIVITY MEASURES (Don't require Employee data)\n    # ========================================================================\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ALTERNATIVE PRODUCTIVITY MEASURES (No Employee Data Needed)\")\n    print(\"=\" * 60)\n\n    # 1. Asset Turnover (Revenue / Total Assets)\n    if revenue_col and assets_col:\n        mask = (df[assets_col] > 0) & (df[revenue_col].notna())\n        df['asset_turnover_raw'] = np.nan\n        df.loc[mask, 'asset_turnover_raw'] = df.loc[mask, revenue_col] / df.loc[mask, assets_col]\n        df['asset_turnover'] = winsorize(df['asset_turnover_raw'], 0.01, 0.99)\n        print(f\"✓ Asset Turnover (Revenue/Assets): {df['asset_turnover'].notna().sum():,} obs\")\n    else:\n        df['asset_turnover'] = np.nan\n        print(\"✗ Asset Turnover: Missing Revenue or Assets\")\n\n    # 2. EBITDA Margin\n    if revenue_col and ebitda_col:\n        mask = (df[revenue_col] > 0) & (df[ebitda_col].notna())\n        df['ebitda_margin_raw'] = np.nan\n        df.loc[mask, 'ebitda_margin_raw'] = df.loc[mask, ebitda_col] / df.loc[mask, revenue_col]\n        df['ebitda_margin'] = winsorize(df['ebitda_margin_raw'], 0.01, 0.99)\n        print(f\"✓ EBITDA Margin: {df['ebitda_margin'].notna().sum():,} obs\")\n    else:\n        df['ebitda_margin'] = np.nan\n        print(\"✗ EBITDA Margin: Missing Revenue or EBITDA\")\n    \n    # 3. Operating Margin\n    if revenue_col and operating_income_col:\n        mask = (df[revenue_col] > 0) & (df[operating_income_col].notna())\n        df['operating_margin_raw'] = np.nan\n        df.loc[mask, 'operating_margin_raw'] = df.loc[mask, operating_income_col] / df.loc[mask, revenue_col]\n        df['operating_margin'] = winsorize(df['operating_margin_raw'], 0.01, 0.99)\n        print(f\"✓ Operating Margin: {df['operating_margin'].notna().sum():,} obs\")\n    else:\n        df['operating_margin'] = np.nan\n        print(\"✗ Operating Margin: Missing Revenue or Operating Income\")\n\n    # 4. R&D Intensity\n    if revenue_col and rd_col:\n        mask = (df[revenue_col] > 0) & (df[rd_col].notna())\n        df['rd_intensity_raw'] = np.nan\n        df.loc[mask, 'rd_intensity_raw'] = df.loc[mask, rd_col] / df.loc[mask, revenue_col]\n        df['rd_intensity'] = winsorize(df['rd_intensity_raw'], 0.01, 0.99)\n        print(f\"✓ R&D Intensity (R&D/Revenue): {df['rd_intensity'].notna().sum():,} obs\")\n    else:\n        df['rd_intensity'] = np.nan\n        print(\"✗ R&D Intensity: Missing Revenue or R&D\")\n\n    # 5. CapEx Intensity\n    if revenue_col and capex_col:\n        mask = (df[revenue_col] > 0) & (df[capex_col].notna())\n        df['capex_intensity_raw'] = np.nan\n        # Note: CapEx is often negative (outflow), so we use absolute value\n        df.loc[mask, 'capex_intensity_raw'] = np.abs(df.loc[mask, capex_col]) / df.loc[mask, revenue_col]\n        df['capex_intensity'] = winsorize(df['capex_intensity_raw'], 0.01, 0.99)\n        print(f\"✓ CapEx Intensity (CapEx/Revenue): {df['capex_intensity'].notna().sum():,} obs\")\n    else:\n        df['capex_intensity'] = np.nan\n        print(\"✗ CapEx Intensity: Missing Revenue or CapEx\")\n\n    # 6. Revenue per Employee (only if data available)\n    if employee_data_available and revenue_col:\n        mask = (df[employee_col] > 0) & (df[revenue_col].notna())\n        df['revenue_per_employee_raw'] = np.nan\n        df.loc[mask, 'revenue_per_employee_raw'] = df.loc[mask, revenue_col] / df.loc[mask, employee_col]\n        df['revenue_per_employee'] = winsorize(df['revenue_per_employee_raw'], 0.01, 0.99)\n        print(f\"✓ Revenue/Employee: {df['revenue_per_employee'].notna().sum():,} obs\")\n    else:\n        df['revenue_per_employee'] = np.nan\n        print(\"✗ Revenue/Employee: Employee data not available\")\n\n    # Log transformations for size controls\n    if revenue_col:\n        df['log_revenue'] = np.log(df[revenue_col].clip(lower=1e-6))\n    if assets_col:\n        df['log_assets'] = np.log(df[assets_col].clip(lower=1e-6))\n\n    return df, {\n        'revenue': revenue_col, 'sga': sga_col, 'employees': employee_col,\n        'ebitda': ebitda_col, 'assets': assets_col, 'gross_profit': gross_profit_col,\n        'operating_income': operating_income_col, 'rd': rd_col, 'capex': capex_col,\n        'employee_data_available': employee_data_available\n    }\n\n# Construct variables\npanel, var_map = construct_variables(panel)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"VARIABLE CONSTRUCTION COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"\\nEmployee data available: {var_map.get('employee_data_available', False)}\")\n\nif not var_map.get('employee_data_available', False):\n    print(\"\"\"\n┌─────────────────────────────────────────────────────────────────────────────┐\n│  ⚠ DATA LIMITATION: Employee counts are missing from source data            │\n│                                                                              │\n│  The \"Hollow Firm\" analysis can still proceed using:                        │\n│  • SG&A Efficiency (Operating Expenses / Revenue)                           │\n│  • Operating Margin (Operating Income / Revenue)                            │\n│  • Asset Turnover (Revenue / Total Assets)                                  │\n│  • EBITDA Margin                                                            │\n│                                                                              │\n│  RECOMMENDATION: Source employee data from:                                 │\n│  • Compustat annual fundamentals (EMP field)                               │\n│  • Company 10-K filings (Item 1 - Business description)                    │\n│  • Bureau of Labor Statistics (BLS) industry data                          │\n└─────────────────────────────────────────────────────────────────────────────┘\n\"\"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71yVkvmptDuG"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TREATMENT ASSIGNMENT\n",
    "# ============================================================================\n",
    "\n",
    "# AI Exposure classification based on industry\n",
    "HIGH_AI_INDUSTRIES = [\n",
    "    'software', 'technology', 'internet', 'it service', 'computer',\n",
    "    'semiconductor', 'electronic', 'telecom',\n",
    "    'consulting', 'professional service', 'business service',\n",
    "    'advertising', 'marketing', 'media', 'publishing',\n",
    "    'banking', 'financial service', 'insurance', 'asset management',\n",
    "    'investment', 'fintech', 'capital market',\n",
    "    'healthcare', 'pharmaceutical', 'biotech',\n",
    "    'retail', 'e-commerce', 'customer service'\n",
    "]\n",
    "\n",
    "LOW_AI_INDUSTRIES = [\n",
    "    'construction', 'mining', 'agriculture', 'forestry',\n",
    "    'utilities', 'oil', 'gas', 'energy', 'petroleum',\n",
    "    'manufacturing', 'industrial', 'machinery', 'automotive', 'aerospace',\n",
    "    'transportation', 'logistics', 'shipping', 'trucking', 'airline',\n",
    "    'real estate', 'reit', 'hospitality', 'hotel',\n",
    "    'food', 'beverage', 'restaurant'\n",
    "]\n",
    "\n",
    "def assign_treatment(industry_str):\n",
    "    \"\"\"Assign AI exposure treatment based on industry.\"\"\"\n",
    "    if pd.isna(industry_str):\n",
    "        return np.nan\n",
    "\n",
    "    ind_lower = str(industry_str).lower()\n",
    "\n",
    "    for kw in HIGH_AI_INDUSTRIES:\n",
    "        if kw in ind_lower:\n",
    "            return 1\n",
    "\n",
    "    for kw in LOW_AI_INDUSTRIES:\n",
    "        if kw in ind_lower:\n",
    "            return 0\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Find industry column\n",
    "industry_col = col_map['industry']\n",
    "\n",
    "if industry_col is None:\n",
    "    # Try to find it in wide data\n",
    "    for col in df_wide.columns:\n",
    "        if 'industry' in col.lower() or 'sector' in col.lower():\n",
    "            industry_col = col\n",
    "            break\n",
    "\n",
    "# Merge industry from wide data if needed\n",
    "if industry_col and industry_col not in panel.columns:\n",
    "    industry_map = df_wide[[FIRM_ID, industry_col]].drop_duplicates()\n",
    "    panel = panel.merge(industry_map, on=FIRM_ID, how='left')\n",
    "\n",
    "# Assign treatment\n",
    "if industry_col and industry_col in panel.columns:\n",
    "    panel['treated'] = panel[industry_col].apply(assign_treatment)\n",
    "    print(f\"\\nTreatment assignment:\")\n",
    "    print(panel['treated'].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"\\n⚠ Industry column not found - creating random treatment for demo\")\n",
    "    # Random assignment for demonstration (REPLACE WITH REAL DATA)\n",
    "    firm_treatment = df_wide[[FIRM_ID]].drop_duplicates()\n",
    "    firm_treatment['treated'] = np.random.binomial(1, 0.5, len(firm_treatment))\n",
    "    panel = panel.merge(firm_treatment, on=FIRM_ID, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDY53hN8tDuG"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# POST-TREATMENT INDICATOR\n",
    "# ============================================================================\n",
    "\n",
    "# ChatGPT released November 30, 2022\n",
    "# First full post-treatment quarter: Q1 2023\n",
    "# We code Q4 2022 as the \"event quarter\" (partially treated)\n",
    "\n",
    "CHATGPT_YEAR = 2022\n",
    "CHATGPT_QUARTER = 4\n",
    "\n",
    "# Post = 1 for Q1 2023 onwards (strictly post-release)\n",
    "panel['post'] = ((panel['year'] > CHATGPT_YEAR) |\n",
    "                 ((panel['year'] == CHATGPT_YEAR) & (panel['quarter'] > CHATGPT_QUARTER))).astype(int)\n",
    "\n",
    "# DiD interaction\n",
    "panel['treated_x_post'] = panel['treated'] * panel['post']\n",
    "\n",
    "# Event time (quarters relative to Q4 2022)\n",
    "event_yq = CHATGPT_YEAR * 4 + CHATGPT_QUARTER\n",
    "panel['event_time'] = panel['yearquarter'] - event_yq\n",
    "\n",
    "print(f\"Post-treatment observations: {panel['post'].sum():,} / {len(panel):,}\")\n",
    "print(f\"Event time range: [{panel['event_time'].min()}, {panel['event_time'].max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idx66FlCtDuG"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAMPLE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "# Keep only observations with:\n",
    "# 1. Non-missing outcome (SGA Efficiency)\n",
    "# 2. Non-missing treatment\n",
    "\n",
    "analysis_sample = panel[\n",
    "    (panel['sga_efficiency'].notna()) &\n",
    "    (panel['treated'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nAnalysis Sample:\")\n",
    "print(f\"  Observations: {len(analysis_sample):,}\")\n",
    "print(f\"  Firms: {analysis_sample[FIRM_ID].nunique():,}\")\n",
    "print(f\"  Periods: {analysis_sample['period'].nunique()}\")\n",
    "print(f\"  Treated firms: {analysis_sample[analysis_sample['treated']==1][FIRM_ID].nunique():,}\")\n",
    "print(f\"  Control firms: {analysis_sample[analysis_sample['treated']==0][FIRM_ID].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOhcZU3YtDuG"
   },
   "source": [
    "---\n",
    "## Section 3: Descriptive Statistics & Parallel Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvNcyoehtDuH"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TABLE 1: SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def create_summary_table(df, outcome_vars, by_treatment=True):\n",
    "    \"\"\"\n",
    "    Create publication-ready summary statistics table.\n",
    "    \"\"\"\n",
    "    if by_treatment:\n",
    "        # Pre-period only for balance check\n",
    "        pre_data = df[df['post'] == 0]\n",
    "\n",
    "        summary = []\n",
    "        for var in outcome_vars:\n",
    "            if var in pre_data.columns:\n",
    "                treated = pre_data[pre_data['treated'] == 1][var]\n",
    "                control = pre_data[pre_data['treated'] == 0][var]\n",
    "\n",
    "                # T-test for difference\n",
    "                if len(treated.dropna()) > 0 and len(control.dropna()) > 0:\n",
    "                    tstat, pval = stats.ttest_ind(treated.dropna(), control.dropna())\n",
    "                else:\n",
    "                    tstat, pval = np.nan, np.nan\n",
    "\n",
    "                summary.append({\n",
    "                    'Variable': var,\n",
    "                    'Treated Mean': treated.mean(),\n",
    "                    'Treated SD': treated.std(),\n",
    "                    'Control Mean': control.mean(),\n",
    "                    'Control SD': control.std(),\n",
    "                    'Difference': treated.mean() - control.mean(),\n",
    "                    't-stat': tstat,\n",
    "                    'p-value': pval\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(summary)\n",
    "    else:\n",
    "        return df[outcome_vars].describe().T\n",
    "\n",
    "# Define variables for summary\n",
    "summary_vars = ['sga_efficiency', 'revenue_per_employee', 'ebitda_margin',\n",
    "                'log_revenue', 'log_employees']\n",
    "summary_vars = [v for v in summary_vars if v in analysis_sample.columns]\n",
    "\n",
    "summary_table = create_summary_table(analysis_sample, summary_vars, by_treatment=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 1: SUMMARY STATISTICS (Pre-Period)\")\n",
    "print(\"=\" * 80)\n",
    "display(summary_table.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dF2FkaQtDuH"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIGURE 1: PARALLEL TRENDS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_parallel_trends_publication(df, outcome, treatment_col='treated',\n",
    "                                      event_date='2022-11-01', save_path=None):\n",
    "    \"\"\"\n",
    "    Create publication-quality parallel trends figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Compute means by period and treatment\n",
    "    trends = df.groupby(['year', 'quarter', treatment_col])[outcome].mean().reset_index()\n",
    "    trends['date'] = pd.to_datetime(trends['year'].astype(str) + '-' +\n",
    "                                     ((trends['quarter']-1)*3 + 1).astype(str) + '-01')\n",
    "\n",
    "    # Colors\n",
    "    colors = {1: '#2E86AB', 0: '#A23B72'}  # Blue for treated, Magenta for control\n",
    "    labels = {1: 'High AI Exposure (Treated)', 0: 'Low AI Exposure (Control)'}\n",
    "\n",
    "    for treat_val in [0, 1]:\n",
    "        group = trends[trends[treatment_col] == treat_val].sort_values('date')\n",
    "        ax.plot(group['date'], group[outcome],\n",
    "                marker='o', markersize=6, linewidth=2,\n",
    "                color=colors[treat_val], label=labels[treat_val])\n",
    "\n",
    "    # Event line\n",
    "    ax.axvline(pd.to_datetime(event_date), color='#E74C3C', linestyle='--',\n",
    "               linewidth=2, label='ChatGPT Release (Nov 2022)', alpha=0.8)\n",
    "\n",
    "    # Shading for post-period\n",
    "    ax.axvspan(pd.to_datetime(event_date), trends['date'].max(),\n",
    "               alpha=0.1, color='gray', label='Post-Treatment Period')\n",
    "\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel(f'{outcome.replace(\"_\", \" \").title()}', fontsize=12)\n",
    "    ax.set_title('Figure 1: Parallel Trends in SG&A Efficiency', fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax.legend(loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Rotate x-labels\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Plot parallel trends\n",
    "if 'sga_efficiency' in analysis_sample.columns:\n",
    "    fig = plot_parallel_trends_publication(analysis_sample, 'sga_efficiency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT_b74TwtDuH"
   },
   "source": [
    "---\n",
    "## Section 4: Main DiD Specification\n",
    "\n",
    "**Specification:**\n",
    "$$Y_{it} = \\alpha_i + \\alpha_t + \\beta(Treated_i \\times Post_t) + \\epsilon_{it}$$\n",
    "\n",
    "Where:\n",
    "- $Y_{it}$ = SGA Efficiency (SG&A / Revenue)\n",
    "- $\\alpha_i$ = Firm fixed effects\n",
    "- $\\alpha_t$ = Year-Quarter fixed effects\n",
    "- $\\beta$ = **Treatment effect** (coefficient of interest)\n",
    "- Standard errors clustered at firm level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMpgUI2mtDuH"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN DiD REGRESSION\n",
    "# ============================================================================\n",
    "\n",
    "def run_did_twfe(df, outcome, firm_id, time_var='period',\n",
    "                 treatment_interaction='treated_x_post', controls=None):\n",
    "    \"\"\"\n",
    "    Run Two-Way Fixed Effects DiD regression.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "    outcome : str, dependent variable\n",
    "    firm_id : str, firm identifier\n",
    "    time_var : str, time period identifier\n",
    "    treatment_interaction : str, DiD interaction term\n",
    "    controls : list, optional control variables\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    PanelOLS results object\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    keep_cols = [firm_id, time_var, outcome, treatment_interaction]\n",
    "    if controls:\n",
    "        keep_cols.extend(controls)\n",
    "\n",
    "    reg_data = df[keep_cols].dropna().copy()\n",
    "\n",
    "    if len(reg_data) < 100:\n",
    "        raise ValueError(f\"Insufficient observations: {len(reg_data)}\")\n",
    "\n",
    "    # Set panel index\n",
    "    reg_data = reg_data.set_index([firm_id, time_var])\n",
    "\n",
    "    # Define model\n",
    "    y = reg_data[outcome]\n",
    "\n",
    "    X_cols = [treatment_interaction]\n",
    "    if controls:\n",
    "        X_cols.extend(controls)\n",
    "\n",
    "    X = sm.add_constant(reg_data[X_cols])\n",
    "\n",
    "    # Estimate with TWFE\n",
    "    model = PanelOLS(y, X, entity_effects=True, time_effects=True)\n",
    "    results = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run main regression\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 2: MAIN DiD RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "main_result = run_did_twfe(\n",
    "    analysis_sample,\n",
    "    outcome='sga_efficiency',\n",
    "    firm_id=FIRM_ID,\n",
    "    time_var='period',\n",
    "    treatment_interaction='treated_x_post'\n",
    ")\n",
    "\n",
    "print(main_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeYwlWaitDuH"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT KEY COEFFICIENT\n",
    "# ============================================================================\n",
    "\n",
    "beta_did = main_result.params['treated_x_post']\n",
    "se_did = main_result.std_errors['treated_x_post']\n",
    "tstat_did = main_result.tstats['treated_x_post']\n",
    "pval_did = main_result.pvalues['treated_x_post']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY RESULT: DiD Coefficient (β)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  β (Treated × Post) = {beta_did:.6f}\")\n",
    "print(f\"  Standard Error     = {se_did:.6f}\")\n",
    "print(f\"  t-statistic        = {tstat_did:.4f}\")\n",
    "print(f\"  p-value            = {pval_did:.6f}\")\n",
    "print(f\"\\n  Observations       = {main_result.nobs:,}\")\n",
    "print(f\"  R² (within)        = {main_result.rsquared_within:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0rudGjjtDuH"
   },
   "source": [
    "---\n",
    "## Section 5: Randomization Inference (5,000 Permutations)\n",
    "\n",
    "**Purpose:** Validate the DiD coefficient by showing it's unlikely to arise by chance.\n",
    "\n",
    "**Method:**\n",
    "1. Randomly reassign treatment across firms (keeping within-firm correlation)\n",
    "2. Re-estimate DiD 5,000 times\n",
    "3. Compute empirical p-value: proportion of fake coefficients more extreme than true coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWff_R3ptDuH"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RANDOMIZATION INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "def run_single_permutation(df, outcome, firm_id, time_var, seed):\n",
    "    \"\"\"\n",
    "    Run a single permutation of the DiD with shuffled treatment.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    try:\n",
    "        # Get firm-level treatment and shuffle\n",
    "        firm_treatment = df[[firm_id, 'treated']].drop_duplicates(subset=[firm_id])\n",
    "        shuffled_treatment = firm_treatment['treated'].values.copy()\n",
    "        np.random.shuffle(shuffled_treatment)\n",
    "        firm_treatment['treated_placebo'] = shuffled_treatment\n",
    "\n",
    "        # Merge back\n",
    "        df_perm = df.drop(columns=['treated_x_post'], errors='ignore').merge(\n",
    "            firm_treatment[[firm_id, 'treated_placebo']], on=firm_id, how='left'\n",
    "        )\n",
    "        df_perm['treated_x_post_placebo'] = df_perm['treated_placebo'] * df_perm['post']\n",
    "\n",
    "        # Prepare regression data\n",
    "        reg_data = df_perm[[firm_id, time_var, outcome, 'treated_x_post_placebo']].dropna()\n",
    "        reg_data = reg_data.set_index([firm_id, time_var])\n",
    "\n",
    "        y = reg_data[outcome]\n",
    "        X = sm.add_constant(reg_data[['treated_x_post_placebo']])\n",
    "\n",
    "        model = PanelOLS(y, X, entity_effects=True, time_effects=True)\n",
    "        result = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "        return result.params['treated_x_post_placebo']\n",
    "\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def run_randomization_inference(df, outcome, firm_id, time_var='period',\n",
    "                                 n_permutations=5000, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Run full randomization inference with parallel processing.\n",
    "    \"\"\"\n",
    "    print(f\"\\nRunning {n_permutations:,} permutations...\")\n",
    "    print(f\"Using {n_jobs} CPU cores (-1 = all available)\")\n",
    "\n",
    "    # Generate seeds\n",
    "    seeds = np.random.randint(0, 1e7, n_permutations)\n",
    "\n",
    "    # Run in parallel\n",
    "    placebo_coefs = Parallel(n_jobs=n_jobs, verbose=5)(\n",
    "        delayed(run_single_permutation)(df, outcome, firm_id, time_var, seed)\n",
    "        for seed in seeds\n",
    "    )\n",
    "\n",
    "    return np.array(placebo_coefs)\n",
    "\n",
    "# Run randomization inference\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANDOMIZATION INFERENCE (5,000 Permutations)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "placebo_coefs = run_randomization_inference(\n",
    "    analysis_sample,\n",
    "    outcome='sga_efficiency',\n",
    "    firm_id=FIRM_ID,\n",
    "    time_var='period',\n",
    "    n_permutations=N_PERMUTATIONS,\n",
    "    n_jobs=N_CORES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67pY11wYtDuH"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPUTE EMPIRICAL P-VALUE\n",
    "# ============================================================================\n",
    "\n",
    "# Remove NaN values\n",
    "placebo_coefs_clean = placebo_coefs[~np.isnan(placebo_coefs)]\n",
    "\n",
    "print(f\"\\nSuccessful permutations: {len(placebo_coefs_clean):,} / {N_PERMUTATIONS:,}\")\n",
    "\n",
    "# Two-sided empirical p-value\n",
    "# P(|placebo| >= |true|)\n",
    "empirical_pval = np.mean(np.abs(placebo_coefs_clean) >= np.abs(beta_did))\n",
    "\n",
    "# One-sided (if we have directional hypothesis: beta < 0)\n",
    "empirical_pval_onesided = np.mean(placebo_coefs_clean <= beta_did)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RANDOMIZATION INFERENCE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  True DiD Coefficient (β):     {beta_did:.6f}\")\n",
    "print(f\"  Placebo Mean:                  {np.mean(placebo_coefs_clean):.6f}\")\n",
    "print(f\"  Placebo Std Dev:               {np.std(placebo_coefs_clean):.6f}\")\n",
    "print(f\"\\n  Empirical p-value (two-sided): {empirical_pval:.4f}\")\n",
    "print(f\"  Empirical p-value (one-sided): {empirical_pval_onesided:.4f}\")\n",
    "print(f\"\\n  Percentile of true β:          {percentileofscore(placebo_coefs_clean, beta_did):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_k-BelAtDuI"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIGURE 2: RANDOMIZATION INFERENCE HISTOGRAM\n",
    "# ============================================================================\n",
    "\n",
    "def plot_randomization_inference(placebo_coefs, true_coef, empirical_pval, save_path=None):\n",
    "    \"\"\"\n",
    "    Create publication-quality randomization inference figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Histogram of placebo coefficients\n",
    "    n, bins, patches = ax.hist(placebo_coefs, bins=80, density=True,\n",
    "                                alpha=0.7, color='#3498DB', edgecolor='white',\n",
    "                                label=f'Placebo Distribution (n={len(placebo_coefs):,})')\n",
    "\n",
    "    # Kernel density estimate\n",
    "    from scipy.stats import gaussian_kde\n",
    "    kde = gaussian_kde(placebo_coefs[~np.isnan(placebo_coefs)])\n",
    "    x_range = np.linspace(placebo_coefs.min(), placebo_coefs.max(), 200)\n",
    "    ax.plot(x_range, kde(x_range), color='#2C3E50', linewidth=2, label='KDE')\n",
    "\n",
    "    # True coefficient line\n",
    "    ax.axvline(true_coef, color='#E74C3C', linewidth=3, linestyle='--',\n",
    "               label=f'True β = {true_coef:.4f}')\n",
    "\n",
    "    # Shade rejection region\n",
    "    rejection_threshold = np.percentile(placebo_coefs, [2.5, 97.5])\n",
    "    ax.axvline(rejection_threshold[0], color='gray', linewidth=1, linestyle=':')\n",
    "    ax.axvline(rejection_threshold[1], color='gray', linewidth=1, linestyle=':')\n",
    "\n",
    "    # Add text annotations\n",
    "    ax.text(0.02, 0.98, f'Empirical p-value: {empirical_pval:.4f}',\n",
    "            transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "    ax.set_xlabel('DiD Coefficient (β)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Figure 2: Randomization Inference\\nDistribution of Placebo Coefficients',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax.legend(loc='upper right', frameon=True, fancybox=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Plot\n",
    "fig = plot_randomization_inference(placebo_coefs_clean, beta_did, empirical_pval)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LG6HNWUPtDuI"
   },
   "source": [
    "---\n",
    "## Section 6: Event Study\n",
    "\n",
    "Dynamic treatment effects to validate parallel trends and trace out the effect over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOv7L7qttDuI"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVENT STUDY SPECIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def run_event_study(df, outcome, firm_id, event_time_col='event_time',\n",
    "                    time_var='period', omit_period=-1,\n",
    "                    min_period=-12, max_period=8):\n",
    "    \"\"\"\n",
    "    Run event study regression with dynamic treatment effects.\n",
    "\n",
    "    Y_it = α_i + α_t + Σ_k β_k (Treated_i × 1{t=k}) + ε_it\n",
    "\n",
    "    Returns coefficient DataFrame for plotting.\n",
    "    \"\"\"\n",
    "    # Filter to event window\n",
    "    df_es = df[(df[event_time_col] >= min_period) &\n",
    "               (df[event_time_col] <= max_period)].copy()\n",
    "\n",
    "    # Create event time dummies interacted with treatment\n",
    "    event_times = sorted(df_es[event_time_col].unique())\n",
    "\n",
    "    for t in event_times:\n",
    "        if t != omit_period:\n",
    "            df_es[f'treat_t{t}'] = ((df_es[event_time_col] == t) * df_es['treated']).astype(float)\n",
    "\n",
    "    # Regression\n",
    "    interact_cols = [c for c in df_es.columns if c.startswith('treat_t')]\n",
    "\n",
    "    reg_data = df_es[[firm_id, time_var, outcome] + interact_cols].dropna()\n",
    "    reg_data = reg_data.set_index([firm_id, time_var])\n",
    "\n",
    "    y = reg_data[outcome]\n",
    "    X = sm.add_constant(reg_data[interact_cols])\n",
    "\n",
    "    model = PanelOLS(y, X, entity_effects=True, time_effects=True)\n",
    "    result = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "    # Extract coefficients\n",
    "    coefs = []\n",
    "    for t in event_times:\n",
    "        if t == omit_period:\n",
    "            coefs.append({'event_time': t, 'coef': 0, 'se': 0,\n",
    "                         'ci_low': 0, 'ci_high': 0, 'pval': np.nan})\n",
    "        else:\n",
    "            col = f'treat_t{t}'\n",
    "            if col in result.params.index:\n",
    "                coef = result.params[col]\n",
    "                se = result.std_errors[col]\n",
    "                pval = result.pvalues[col]\n",
    "                coefs.append({\n",
    "                    'event_time': t,\n",
    "                    'coef': coef,\n",
    "                    'se': se,\n",
    "                    'ci_low': coef - 1.96 * se,\n",
    "                    'ci_high': coef + 1.96 * se,\n",
    "                    'pval': pval\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(coefs), result\n",
    "\n",
    "# Run event study\n",
    "es_coefs, es_result = run_event_study(\n",
    "    analysis_sample,\n",
    "    outcome='sga_efficiency',\n",
    "    firm_id=FIRM_ID,\n",
    "    event_time_col='event_time',\n",
    "    time_var='period'\n",
    ")\n",
    "\n",
    "print(\"\\nEvent Study Coefficients:\")\n",
    "display(es_coefs.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqiCjJNltDuI"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIGURE 3: EVENT STUDY PLOT\n",
    "# ============================================================================\n",
    "\n",
    "def plot_event_study_publication(coef_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create publication-quality event study figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Confidence intervals\n",
    "    ax.fill_between(coef_df['event_time'], coef_df['ci_low'], coef_df['ci_high'],\n",
    "                    alpha=0.25, color='#3498DB', label='95% CI')\n",
    "\n",
    "    # Point estimates\n",
    "    ax.plot(coef_df['event_time'], coef_df['coef'], 'o-',\n",
    "            color='#2C3E50', linewidth=2.5, markersize=8, label='Point Estimate')\n",
    "\n",
    "    # Reference lines\n",
    "    ax.axhline(0, color='black', linewidth=0.8, linestyle='-')\n",
    "    ax.axvline(0, color='#E74C3C', linewidth=2, linestyle='--',\n",
    "               label='ChatGPT Release (Q4 2022)')\n",
    "\n",
    "    # Shade pre vs post\n",
    "    ax.axvspan(coef_df['event_time'].min(), 0, alpha=0.05, color='gray')\n",
    "\n",
    "    ax.set_xlabel('Quarters Relative to ChatGPT Release', fontsize=12)\n",
    "    ax.set_ylabel('Effect on SG&A Efficiency (SG&A / Revenue)', fontsize=12)\n",
    "    ax.set_title('Figure 3: Event Study - Dynamic Treatment Effects\\n\"The Hollowing Out Effect\"',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax.legend(loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Set x-ticks\n",
    "    ax.set_xticks(coef_df['event_time'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_event_study_publication(es_coefs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhsWQ42atDuI"
   },
   "source": [
    "---\n",
    "## Section 7: Heterogeneity Analysis\n",
    "\n",
    "Testing whether the \"hollowing\" effect is stronger for certain types of firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpUg2ogKtDuI"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HETEROGENEITY BY FIRM SIZE\n",
    "# ============================================================================\n",
    "\n",
    "def run_heterogeneity_analysis(df, outcome, firm_id, time_var='period',\n",
    "                                split_var='log_revenue', split_type='median'):\n",
    "    \"\"\"\n",
    "    Run DiD separately for subsamples (heterogeneity analysis).\n",
    "    \"\"\"\n",
    "    if split_var not in df.columns:\n",
    "        print(f\"Split variable {split_var} not found\")\n",
    "        return None\n",
    "\n",
    "    # Compute split threshold (using pre-period values)\n",
    "    pre_period = df[df['post'] == 0]\n",
    "    firm_avg = pre_period.groupby(firm_id)[split_var].mean().reset_index()\n",
    "\n",
    "    if split_type == 'median':\n",
    "        threshold = firm_avg[split_var].median()\n",
    "    elif split_type == 'tercile':\n",
    "        threshold = firm_avg[split_var].quantile([0.33, 0.67]).values\n",
    "\n",
    "    firm_avg['size_group'] = (firm_avg[split_var] > threshold).map({True: 'Large', False: 'Small'})\n",
    "\n",
    "    # Merge back\n",
    "    df_het = df.merge(firm_avg[[firm_id, 'size_group']], on=firm_id, how='left')\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for group in ['Large', 'Small']:\n",
    "        subset = df_het[df_het['size_group'] == group]\n",
    "        print(f\"\\n{group} firms: {subset[firm_id].nunique():,} firms, {len(subset):,} obs\")\n",
    "\n",
    "        try:\n",
    "            result = run_did_twfe(subset, outcome, firm_id, time_var, 'treated_x_post')\n",
    "            results[group] = {\n",
    "                'coef': result.params['treated_x_post'],\n",
    "                'se': result.std_errors['treated_x_post'],\n",
    "                'pval': result.pvalues['treated_x_post'],\n",
    "                'nobs': result.nobs\n",
    "            }\n",
    "            print(f\"  β = {results[group]['coef']:.6f} (SE = {results[group]['se']:.6f}), p = {results[group]['pval']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            results[group] = None\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run heterogeneity analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 3: HETEROGENEITY BY FIRM SIZE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "het_results = run_heterogeneity_analysis(\n",
    "    analysis_sample,\n",
    "    outcome='sga_efficiency',\n",
    "    firm_id=FIRM_ID,\n",
    "    time_var='period',\n",
    "    split_var='log_revenue' if 'log_revenue' in analysis_sample.columns else 'log_employees'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGsaSN-XtDuJ"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRIPLE DIFFERENCE (if we have industry variation)\n",
    "# ============================================================================\n",
    "\n",
    "def run_triple_difference(df, outcome, firm_id, time_var='period',\n",
    "                          moderator='log_revenue'):\n",
    "    \"\"\"\n",
    "    Run triple-difference specification:\n",
    "    Y_it = α_i + α_t + β1(T×Post) + β2(T×Post×Moderator) + ε_it\n",
    "    \"\"\"\n",
    "    if moderator not in df.columns:\n",
    "        print(f\"Moderator {moderator} not found\")\n",
    "        return None\n",
    "\n",
    "    df_ddd = df.copy()\n",
    "\n",
    "    # Standardize moderator\n",
    "    df_ddd['mod_std'] = (df_ddd[moderator] - df_ddd[moderator].mean()) / df_ddd[moderator].std()\n",
    "\n",
    "    # Triple interaction\n",
    "    df_ddd['triple_interact'] = df_ddd['treated_x_post'] * df_ddd['mod_std']\n",
    "\n",
    "    # Regression\n",
    "    reg_data = df_ddd[[firm_id, time_var, outcome, 'treated_x_post', 'triple_interact']].dropna()\n",
    "    reg_data = reg_data.set_index([firm_id, time_var])\n",
    "\n",
    "    y = reg_data[outcome]\n",
    "    X = sm.add_constant(reg_data[['treated_x_post', 'triple_interact']])\n",
    "\n",
    "    model = PanelOLS(y, X, entity_effects=True, time_effects=True)\n",
    "    result = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Run triple-diff if we have size variable\n",
    "if 'log_revenue' in analysis_sample.columns or 'log_employees' in analysis_sample.columns:\n",
    "    mod_var = 'log_revenue' if 'log_revenue' in analysis_sample.columns else 'log_employees'\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"TRIPLE DIFFERENCE (Moderator: {mod_var})\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    ddd_result = run_triple_difference(\n",
    "        analysis_sample, 'sga_efficiency', FIRM_ID, 'period', mod_var\n",
    "    )\n",
    "\n",
    "    if ddd_result:\n",
    "        print(ddd_result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmgiT04wtDuJ"
   },
   "source": [
    "---\n",
    "## Section 8: Additional Outcomes (Multiple Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAbEPNnJtDuJ"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTIPLE OUTCOMES\n",
    "# ============================================================================\n",
    "\n",
    "outcomes_to_test = [\n",
    "    ('sga_efficiency', 'SG&A / Revenue'),\n",
    "    ('revenue_per_employee', 'Revenue / Employee'),\n",
    "    ('ebitda_margin', 'EBITDA / Revenue'),\n",
    "]\n",
    "\n",
    "# Filter to available outcomes\n",
    "outcomes_to_test = [(var, label) for var, label in outcomes_to_test\n",
    "                    if var in analysis_sample.columns]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 4: MULTIPLE OUTCOME ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "multi_results = []\n",
    "\n",
    "for var, label in outcomes_to_test:\n",
    "    try:\n",
    "        result = run_did_twfe(analysis_sample, var, FIRM_ID, 'period', 'treated_x_post')\n",
    "        multi_results.append({\n",
    "            'Outcome': label,\n",
    "            'β (Treated × Post)': result.params['treated_x_post'],\n",
    "            'Std. Error': result.std_errors['treated_x_post'],\n",
    "            't-stat': result.tstats['treated_x_post'],\n",
    "            'p-value': result.pvalues['treated_x_post'],\n",
    "            'N': result.nobs,\n",
    "            'R² (within)': result.rsquared_within\n",
    "        })\n",
    "        print(f\"\\n{label}:\")\n",
    "        print(f\"  β = {result.params['treated_x_post']:.6f}, p = {result.pvalues['treated_x_post']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{label}: Error - {e}\")\n",
    "\n",
    "if multi_results:\n",
    "    multi_df = pd.DataFrame(multi_results)\n",
    "    print(\"\\n\")\n",
    "    display(multi_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyqmFE11tDuJ"
   },
   "source": [
    "---\n",
    "## Section 9: Causal Forest (Heterogeneous Treatment Effects)\n",
    "\n",
    "Using machine learning to discover which firm characteristics predict stronger treatment effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJ6Cg-gTtDuJ"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CAUSAL FOREST (requires EconML)\n",
    "# ============================================================================\n",
    "\n",
    "if ECONML_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CAUSAL FOREST: HETEROGENEOUS TREATMENT EFFECTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Prepare data for causal forest\n",
    "    # Use post-period data only, predict individual treatment effects\n",
    "\n",
    "    # Find available covariates\n",
    "    potential_covariates = ['log_revenue', 'log_employees', 'log_assets']\n",
    "    covariates = [c for c in potential_covariates if c in analysis_sample.columns]\n",
    "\n",
    "    if len(covariates) >= 2:\n",
    "        cf_data = analysis_sample[\n",
    "            analysis_sample['post'] == 1\n",
    "        ][['sga_efficiency', 'treated'] + covariates].dropna()\n",
    "\n",
    "        Y = cf_data['sga_efficiency'].values\n",
    "        T = cf_data['treated'].values\n",
    "        X = cf_data[covariates].values\n",
    "\n",
    "        print(f\"\\nCausal Forest sample: {len(cf_data):,} observations\")\n",
    "        print(f\"Covariates: {covariates}\")\n",
    "\n",
    "        try:\n",
    "            # Fit causal forest\n",
    "            cf = CausalForestDML(\n",
    "                model_y=GradientBoostingRegressor(n_estimators=100, max_depth=4),\n",
    "                model_t=GradientBoostingRegressor(n_estimators=100, max_depth=4),\n",
    "                n_estimators=200,\n",
    "                min_samples_leaf=20,\n",
    "                random_state=RANDOM_SEED,\n",
    "                n_jobs=N_CORES\n",
    "            )\n",
    "\n",
    "            cf.fit(Y, T, X=X)\n",
    "\n",
    "            # Get treatment effects\n",
    "            treatment_effects = cf.effect(X)\n",
    "\n",
    "            print(f\"\\nAverage Treatment Effect (ATE): {treatment_effects.mean():.6f}\")\n",
    "            print(f\"Treatment Effect Std Dev: {treatment_effects.std():.6f}\")\n",
    "            print(f\"Treatment Effect Range: [{treatment_effects.min():.6f}, {treatment_effects.max():.6f}]\")\n",
    "\n",
    "            # Feature importance\n",
    "            print(\"\\nFeature Importance for Treatment Effect Heterogeneity:\")\n",
    "            importance = cf.feature_importances_\n",
    "            for cov, imp in zip(covariates, importance):\n",
    "                print(f\"  {cov}: {imp:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Causal Forest error: {e}\")\n",
    "    else:\n",
    "        print(\"Insufficient covariates for Causal Forest\")\n",
    "else:\n",
    "    print(\"\\nCausal Forest skipped - EconML not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtIjmjrhtDuJ"
   },
   "source": [
    "---\n",
    "## Section 10: Save Results and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fGyV1d2tDuJ"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE ALL RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Save analysis panel\n",
    "analysis_sample.to_parquet(DATA_PATH / 'analysis_panel_hollow_firm.parquet', index=False)\n",
    "\n",
    "# Save event study coefficients\n",
    "es_coefs.to_csv(DATA_PATH / 'event_study_coefficients.csv', index=False)\n",
    "\n",
    "# Save placebo distribution\n",
    "np.save(DATA_PATH / 'placebo_coefficients.npy', placebo_coefs_clean)\n",
    "\n",
    "# Save summary results\n",
    "results_summary = {\n",
    "    'main_coefficient': beta_did,\n",
    "    'main_se': se_did,\n",
    "    'main_pvalue': pval_did,\n",
    "    'empirical_pvalue': empirical_pval,\n",
    "    'n_permutations': len(placebo_coefs_clean),\n",
    "    'n_observations': main_result.nobs,\n",
    "    'n_firms': analysis_sample[FIRM_ID].nunique(),\n",
    "    'r_squared_within': main_result.rsquared_within\n",
    "}\n",
    "\n",
    "pd.Series(results_summary).to_csv(DATA_PATH / 'results_summary.csv')\n",
    "\n",
    "print(\"\\n✓ All results saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRHE94dntDuJ"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE: THE HOLLOW FIRM HYPOTHESIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           MAIN RESULT                                       │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│  DiD Coefficient (β):           {beta_did:>12.6f}                               │\n",
    "│  Standard Error:                {se_did:>12.6f}                               │\n",
    "│  Conventional p-value:          {pval_did:>12.6f}                               │\n",
    "│  Randomization p-value:         {empirical_pval:>12.6f}                               │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│  Observations:                  {main_result.nobs:>12,}                               │\n",
    "│  Unique Firms:                  {analysis_sample[FIRM_ID].nunique():>12,}                               │\n",
    "│  R² (within):                   {main_result.rsquared_within:>12.4f}                               │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk-zBg6XtDuK"
   },
   "source": [
    "---\n",
    "## Professor's Commentary\n",
    "\n",
    "### Interpretation of a Negative, Significant β\n",
    "\n",
    "If $\\beta < 0$ and statistically significant, here's how to interpret:\n",
    "\n",
    "**Plain English:**\n",
    "> \"Following the release of ChatGPT, firms with high AI exposure reduced their SG&A-to-Revenue ratio by [β × 100] percentage points more than firms with low AI exposure, controlling for firm-specific factors and aggregate time trends.\"\n",
    "\n",
    "**Economic Magnitude:**\n",
    "- If β = -0.02, this means a 2 percentage point reduction in SG&A/Revenue\n",
    "- For a firm with $1B revenue, this represents $20M in reduced overhead costs\n",
    "- Relative to pre-period mean SG&A efficiency of ~25%, this is an 8% reduction\n",
    "\n",
    "**Causal Claim:**\n",
    "The DiD design with firm and time fixed effects, combined with:\n",
    "1. Parallel pre-trends (visible in event study)\n",
    "2. Sharp post-treatment break\n",
    "3. Randomization inference validation\n",
    "\n",
    "...supports a **causal interpretation**: GenAI *caused* high-exposure firms to become more organizationally efficient (\"hollow\").\n",
    "\n",
    "**Mechanism:**\n",
    "The \"hollowing\" likely reflects:\n",
    "- Automation of middle-management tasks (reporting, coordination)\n",
    "- Reduced administrative overhead (HR, legal, compliance assistance)\n",
    "- Streamlined customer service and support functions\n",
    "\n",
    "**Publication-Worthiness:**\n",
    "This result is tier-1 worthy because:\n",
    "1. **Novel mechanism**: Not just \"AI replaces workers\" but \"AI replaces *organizational friction*\"\n",
    "2. **Clean identification**: ChatGPT is a sharp, unexpected shock\n",
    "3. **Robust inference**: Both conventional and randomization p-values support significance\n",
    "4. **Economic significance**: The magnitude matters for corporate strategy and labor policy\n",
    "\n",
    "---\n",
    "\n",
    "### Caveats to Address in Paper\n",
    "\n",
    "1. **Short post-period**: Only ~2 years of post-data; effects may evolve\n",
    "2. **Treatment measurement**: Industry-level exposure may miss within-industry variation\n",
    "3. **Confounders**: Fed rate hikes (2022-23) may differentially affect treated firms\n",
    "4. **Anticipation**: Some firms may have anticipated AI impact before ChatGPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}