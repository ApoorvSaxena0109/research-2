{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hollow Firm: GenAI and Corporate Organizational Efficiency\n",
    "\n",
    "## Publication-Ready Analysis Notebook\n",
    "\n",
    "**Research Question:** Did generative AI (post-November 2022) enable high-exposure firms to \"hollow out\" their organizational structure, decoupling revenue growth from overhead costs?\n",
    "\n",
    "**Hypothesis:** High-AI-exposure firms reduced their SG&A-to-Revenue ratio (\"corporate bloat\") relative to low-exposure firms after ChatGPT's release.\n",
    "\n",
    "**Identification:** Difference-in-Differences with firm and time fixed effects, validated through randomization inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Structure\n",
    "1. Setup & Data Loading\n",
    "2. Variable Construction (SGA Efficiency, Winsorization)\n",
    "3. Main DiD Specification\n",
    "4. Randomization Inference (5,000 Permutations)\n",
    "5. Heterogeneity Analysis\n",
    "6. Advanced: Synthetic Control & Causal Forest\n",
    "7. Publication-Ready Tables & Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 0: ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install linearmodels pyfixest econml doubleml pysyncon joblib tqdm -q\n",
    "print(\"✓ Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Econometrics\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.panel import PanelOLS\n",
    "from scipy import stats\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Parallel processing (utilize Colab Pro cores)\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Advanced causal inference\n",
    "try:\n",
    "    from econml.dml import CausalForestDML\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    ECONML_AVAILABLE = True\n",
    "except:\n",
    "    ECONML_AVAILABLE = False\n",
    "    print(\"⚠ EconML not available - Causal Forest will be skipped\")\n",
    "\n",
    "# Configuration\n",
    "N_CORES = multiprocessing.cpu_count()\n",
    "N_PERMUTATIONS = 5000  # Randomization inference iterations\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plot style for publication\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Data path\n",
    "DATA_PATH = Path('/content/drive/MyDrive/Paper_2')\n",
    "\n",
    "print(f\"✓ Libraries loaded\")\n",
    "print(f\"✓ CPU cores available: {N_CORES}\")\n",
    "print(f\"✓ Permutation iterations: {N_PERMUTATIONS:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading and Panel Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_combine_data(data_path):\n",
    "    \"\"\"\n",
    "    Load and combine the two Excel files.\n",
    "    Returns a single DataFrame.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_excel(data_path / 'Data_1.xlsx')\n",
    "    df2 = pd.read_excel(data_path / 'Data_2.xlsx')\n",
    "    \n",
    "    print(f\"Data_1 shape: {df1.shape}\")\n",
    "    print(f\"Data_2 shape: {df2.shape}\")\n",
    "    \n",
    "    # Check for common columns to merge on\n",
    "    common_cols = set(df1.columns) & set(df2.columns)\n",
    "    \n",
    "    if common_cols:\n",
    "        print(f\"Common columns for merge: {common_cols}\")\n",
    "        df = pd.merge(df1, df2, on=list(common_cols), how='outer')\n",
    "    elif len(df1) == len(df2):\n",
    "        print(\"Same row count - concatenating horizontally\")\n",
    "        df = pd.concat([df1, df2], axis=1)\n",
    "    else:\n",
    "        print(\"Using Data_1 as primary\")\n",
    "        df = df1\n",
    "    \n",
    "    print(f\"Combined shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df_wide = load_and_combine_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COLUMN IDENTIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def identify_columns(df):\n",
    "    \"\"\"\n",
    "    Automatically identify key columns in the dataset.\n",
    "    \"\"\"\n",
    "    columns = {\n",
    "        'firm_id': None,\n",
    "        'firm_name': None,\n",
    "        'industry': None,\n",
    "        'revenue': [],\n",
    "        'sga': [],\n",
    "        'employees': [],\n",
    "        'ebitda': [],\n",
    "        'total_assets': [],\n",
    "        'intangibles': [],\n",
    "        'market_cap': []\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        \n",
    "        # Identifiers\n",
    "        if 'ticker' in col_lower or 'symbol' in col_lower:\n",
    "            columns['firm_id'] = col\n",
    "        elif 'company' in col_lower and 'name' in col_lower:\n",
    "            columns['firm_name'] = col\n",
    "        elif 'industry' in col_lower or 'sector' in col_lower:\n",
    "            columns['industry'] = col\n",
    "        \n",
    "        # Financial metrics (time series)\n",
    "        elif 'revenue' in col_lower or 'sales' in col_lower:\n",
    "            columns['revenue'].append(col)\n",
    "        elif 'sg&a' in col_lower or 'sga' in col_lower or 'selling' in col_lower:\n",
    "            columns['sga'].append(col)\n",
    "        elif 'employee' in col_lower:\n",
    "            columns['employees'].append(col)\n",
    "        elif 'ebitda' in col_lower:\n",
    "            columns['ebitda'].append(col)\n",
    "        elif 'total asset' in col_lower:\n",
    "            columns['total_assets'].append(col)\n",
    "        elif 'intangible' in col_lower:\n",
    "            columns['intangibles'].append(col)\n",
    "        elif 'market cap' in col_lower:\n",
    "            columns['market_cap'].append(col)\n",
    "    \n",
    "    return columns\n",
    "\n",
    "col_map = identify_columns(df_wide)\n",
    "\n",
    "print(\"\\nIdentified Columns:\")\n",
    "print(\"=\" * 60)\n",
    "for key, val in col_map.items():\n",
    "    if isinstance(val, list):\n",
    "        print(f\"{key}: {len(val)} columns found\")\n",
    "        if val:\n",
    "            print(f\"   Sample: {val[:3]}\")\n",
    "    else:\n",
    "        print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WIDE TO LONG PANEL CONVERSION\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_time_from_column(col_name):\n",
    "    \"\"\"\n",
    "    Extract metric name and time offset from column name.\n",
    "    \n",
    "    Returns: (metric_name, time_offset, time_type)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        (r'(.+?)\\s*\\[LTM(?:\\s*-\\s*(\\d+))?\\]', 'LTM'),\n",
    "        (r'(.+?)\\s*\\[Latest\\s*Quarter(?:\\s*-\\s*(\\d+))?\\]', 'Quarterly'),\n",
    "        (r'(.+?)\\s*\\[Latest(?:\\s*-\\s*(\\d+)\\s*Year)?', 'Annual'),\n",
    "    ]\n",
    "    \n",
    "    for pattern, time_type in patterns:\n",
    "        match = re.search(pattern, col_name, re.IGNORECASE)\n",
    "        if match:\n",
    "            metric = match.group(1).strip()\n",
    "            offset = int(match.group(2)) if match.group(2) else 0\n",
    "            return (metric, offset, time_type)\n",
    "    \n",
    "    return (col_name, None, None)\n",
    "\n",
    "\n",
    "def create_long_panel(df_wide, firm_id_col, time_type='LTM', \n",
    "                      base_year=2024, base_quarter=4):\n",
    "    \"\"\"\n",
    "    Convert wide-format data to long panel format.\n",
    "    \"\"\"\n",
    "    # Parse all columns\n",
    "    parsed_cols = []\n",
    "    for col in df_wide.columns:\n",
    "        metric, offset, ttype = parse_time_from_column(col)\n",
    "        parsed_cols.append({\n",
    "            'original': col,\n",
    "            'metric': metric,\n",
    "            'offset': offset,\n",
    "            'type': ttype\n",
    "        })\n",
    "    \n",
    "    col_df = pd.DataFrame(parsed_cols)\n",
    "    time_cols = col_df[col_df['type'] == time_type]\n",
    "    \n",
    "    print(f\"Found {len(time_cols)} {time_type} columns\")\n",
    "    \n",
    "    # Get time offsets\n",
    "    offsets = sorted(time_cols['offset'].dropna().unique())\n",
    "    print(f\"Time offsets: {offsets}\")\n",
    "    \n",
    "    # Build panel\n",
    "    panels = []\n",
    "    \n",
    "    for offset in offsets:\n",
    "        period_cols = time_cols[time_cols['offset'] == offset]\n",
    "        col_mapping = dict(zip(period_cols['original'], period_cols['metric']))\n",
    "        \n",
    "        # Select available columns\n",
    "        cols_to_use = [firm_id_col] + [c for c in col_mapping.keys() if c in df_wide.columns]\n",
    "        \n",
    "        if len(cols_to_use) <= 1:\n",
    "            continue\n",
    "        \n",
    "        period_df = df_wide[cols_to_use].copy()\n",
    "        period_df = period_df.rename(columns=col_mapping)\n",
    "        period_df['time_offset'] = offset\n",
    "        \n",
    "        # Convert offset to calendar time\n",
    "        total_q = base_year * 4 + base_quarter - offset\n",
    "        period_df['year'] = (total_q - 1) // 4\n",
    "        period_df['quarter'] = ((total_q - 1) % 4) + 1\n",
    "        \n",
    "        panels.append(period_df)\n",
    "    \n",
    "    if not panels:\n",
    "        raise ValueError(f\"No panels created for time_type={time_type}\")\n",
    "    \n",
    "    panel = pd.concat(panels, ignore_index=True)\n",
    "    panel = panel.sort_values([firm_id_col, 'time_offset']).reset_index(drop=True)\n",
    "    \n",
    "    # Create period identifier\n",
    "    panel['period'] = panel['year'].astype(str) + 'Q' + panel['quarter'].astype(str)\n",
    "    panel['yearquarter'] = panel['year'] * 4 + panel['quarter']\n",
    "    \n",
    "    return panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create firm ID if not found\n",
    "FIRM_ID = col_map['firm_id']\n",
    "if FIRM_ID is None:\n",
    "    df_wide['firm_id'] = range(len(df_wide))\n",
    "    FIRM_ID = 'firm_id'\n",
    "    print(\"Created numeric firm_id\")\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATE THESE VALUES BASED ON YOUR DATA\n",
    "# ============================================================================\n",
    "BASE_YEAR = 2024      # What year does \"Latest\" refer to?\n",
    "BASE_QUARTER = 4      # What quarter? (1-4)\n",
    "\n",
    "# Create panel\n",
    "panel = create_long_panel(df_wide, FIRM_ID, time_type='LTM',\n",
    "                          base_year=BASE_YEAR, base_quarter=BASE_QUARTER)\n",
    "\n",
    "print(f\"\\nPanel created: {panel.shape}\")\n",
    "print(f\"Firms: {panel[FIRM_ID].nunique():,}\")\n",
    "print(f\"Periods: {panel['period'].nunique()}\")\n",
    "print(f\"Time range: {panel['year'].min()}-Q{panel[panel['year']==panel['year'].min()]['quarter'].min()} to {panel['year'].max()}-Q{panel[panel['year']==panel['year'].max()]['quarter'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview panel\n",
    "print(\"\\nPanel columns:\")\n",
    "print(panel.columns.tolist())\n",
    "print(\"\\nSample rows:\")\n",
    "display(panel.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Variable Construction\n",
    "\n",
    "**Key Variable:** SGA Efficiency = SG&A / Revenue\n",
    "\n",
    "This measures \"corporate bloat\" - how much overhead is required per dollar of revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: VARIABLE CONSTRUCTION\n",
    "# ============================================================================\n",
    "\n",
    "def winsorize(series, lower=0.01, upper=0.99):\n",
    "    \"\"\"\n",
    "    Winsorize a series at specified percentiles.\n",
    "    Standard practice: 1st and 99th percentiles.\n",
    "    \"\"\"\n",
    "    lower_bound = series.quantile(lower)\n",
    "    upper_bound = series.quantile(upper)\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "\n",
    "def construct_variables(panel):\n",
    "    \"\"\"\n",
    "    Construct all analysis variables with proper handling.\n",
    "    \"\"\"\n",
    "    df = panel.copy()\n",
    "    \n",
    "    # Identify Revenue and SG&A columns (they may have been renamed)\n",
    "    revenue_col = None\n",
    "    sga_col = None\n",
    "    employee_col = None\n",
    "    ebitda_col = None\n",
    "    assets_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if ('revenue' in col_lower or 'sales' in col_lower) and revenue_col is None:\n",
    "            revenue_col = col\n",
    "        elif ('sg&a' in col_lower or 'sga' in col_lower or 'selling' in col_lower) and sga_col is None:\n",
    "            sga_col = col\n",
    "        elif 'employee' in col_lower and employee_col is None:\n",
    "            employee_col = col\n",
    "        elif 'ebitda' in col_lower and ebitda_col is None:\n",
    "            ebitda_col = col\n",
    "        elif 'total asset' in col_lower and assets_col is None:\n",
    "            assets_col = col\n",
    "    \n",
    "    print(f\"Revenue column: {revenue_col}\")\n",
    "    print(f\"SG&A column: {sga_col}\")\n",
    "    print(f\"Employee column: {employee_col}\")\n",
    "    print(f\"EBITDA column: {ebitda_col}\")\n",
    "    print(f\"Assets column: {assets_col}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # KEY VARIABLE: SGA EFFICIENCY (Corporate Bloat Measure)\n",
    "    # ========================================================================\n",
    "    if revenue_col and sga_col:\n",
    "        # Only compute where Revenue > 0 (avoid division issues)\n",
    "        mask = (df[revenue_col] > 0) & (df[sga_col].notna())\n",
    "        df['sga_efficiency_raw'] = np.nan\n",
    "        df.loc[mask, 'sga_efficiency_raw'] = df.loc[mask, sga_col] / df.loc[mask, revenue_col]\n",
    "        \n",
    "        # Winsorize at 1% and 99%\n",
    "        df['sga_efficiency'] = winsorize(df['sga_efficiency_raw'], 0.01, 0.99)\n",
    "        \n",
    "        print(f\"\\nSGA Efficiency constructed: {df['sga_efficiency'].notna().sum():,} obs\")\n",
    "        print(f\"  Raw range: [{df['sga_efficiency_raw'].min():.4f}, {df['sga_efficiency_raw'].max():.4f}]\")\n",
    "        print(f\"  Winsorized range: [{df['sga_efficiency'].min():.4f}, {df['sga_efficiency'].max():.4f}]\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Cannot construct SGA Efficiency - missing Revenue or SG&A\")\n",
    "        df['sga_efficiency'] = np.nan\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SECONDARY VARIABLES\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Revenue per Employee (Productivity)\n",
    "    if revenue_col and employee_col:\n",
    "        mask = (df[employee_col] > 0) & (df[revenue_col].notna())\n",
    "        df['revenue_per_employee_raw'] = np.nan\n",
    "        df.loc[mask, 'revenue_per_employee_raw'] = df.loc[mask, revenue_col] / df.loc[mask, employee_col]\n",
    "        df['revenue_per_employee'] = winsorize(df['revenue_per_employee_raw'], 0.01, 0.99)\n",
    "        print(f\"Revenue/Employee constructed: {df['revenue_per_employee'].notna().sum():,} obs\")\n",
    "    \n",
    "    # EBITDA Margin\n",
    "    if revenue_col and ebitda_col:\n",
    "        mask = (df[revenue_col] > 0) & (df[ebitda_col].notna())\n",
    "        df['ebitda_margin_raw'] = np.nan\n",
    "        df.loc[mask, 'ebitda_margin_raw'] = df.loc[mask, ebitda_col] / df.loc[mask, revenue_col]\n",
    "        df['ebitda_margin'] = winsorize(df['ebitda_margin_raw'], 0.01, 0.99)\n",
    "        print(f\"EBITDA Margin constructed: {df['ebitda_margin'].notna().sum():,} obs\")\n",
    "    \n",
    "    # Log transformations for size controls\n",
    "    if revenue_col:\n",
    "        df['log_revenue'] = np.log(df[revenue_col].clip(lower=1e-6))\n",
    "    if assets_col:\n",
    "        df['log_assets'] = np.log(df[assets_col].clip(lower=1e-6))\n",
    "    if employee_col:\n",
    "        df['log_employees'] = np.log(df[employee_col].clip(lower=1))\n",
    "    \n",
    "    return df, {'revenue': revenue_col, 'sga': sga_col, 'employees': employee_col, \n",
    "                'ebitda': ebitda_col, 'assets': assets_col}\n",
    "\n",
    "# Construct variables\n",
    "panel, var_map = construct_variables(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TREATMENT ASSIGNMENT\n",
    "# ============================================================================\n",
    "\n",
    "# AI Exposure classification based on industry\n",
    "HIGH_AI_INDUSTRIES = [\n",
    "    'software', 'technology', 'internet', 'it service', 'computer',\n",
    "    'semiconductor', 'electronic', 'telecom',\n",
    "    'consulting', 'professional service', 'business service',\n",
    "    'advertising', 'marketing', 'media', 'publishing',\n",
    "    'banking', 'financial service', 'insurance', 'asset management',\n",
    "    'investment', 'fintech', 'capital market',\n",
    "    'healthcare', 'pharmaceutical', 'biotech',\n",
    "    'retail', 'e-commerce', 'customer service'\n",
    "]\n",
    "\n",
    "LOW_AI_INDUSTRIES = [\n",
    "    'construction', 'mining', 'agriculture', 'forestry',\n",
    "    'utilities', 'oil', 'gas', 'energy', 'petroleum',\n",
    "    'manufacturing', 'industrial', 'machinery', 'automotive', 'aerospace',\n",
    "    'transportation', 'logistics', 'shipping', 'trucking', 'airline',\n",
    "    'real estate', 'reit', 'hospitality', 'hotel',\n",
    "    'food', 'beverage', 'restaurant'\n",
    "]\n",
    "\n",
    "def assign_treatment(industry_str):\n",
    "    \"\"\"Assign AI exposure treatment based on industry.\"\"\"\n",
    "    if pd.isna(industry_str):\n",
    "        return np.nan\n",
    "    \n",
    "    ind_lower = str(industry_str).lower()\n",
    "    \n",
    "    for kw in HIGH_AI_INDUSTRIES:\n",
    "        if kw in ind_lower:\n",
    "            return 1\n",
    "    \n",
    "    for kw in LOW_AI_INDUSTRIES:\n",
    "        if kw in ind_lower:\n",
    "            return 0\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "# Find industry column\n",
    "industry_col = col_map['industry']\n",
    "\n",
    "if industry_col is None:\n",
    "    # Try to find it in wide data\n",
    "    for col in df_wide.columns:\n",
    "        if 'industry' in col.lower() or 'sector' in col.lower():\n",
    "            industry_col = col\n",
    "            break\n",
    "\n",
    "# Merge industry from wide data if needed\n",
    "if industry_col and industry_col not in panel.columns:\n",
    "    industry_map = df_wide[[FIRM_ID, industry_col]].drop_duplicates()\n",
    "    panel = panel.merge(industry_map, on=FIRM_ID, how='left')\n",
    "\n",
    "# Assign treatment\n",
    "if industry_col and industry_col in panel.columns:\n",
    "    panel['treated'] = panel[industry_col].apply(assign_treatment)\n",
    "    print(f\"\\nTreatment assignment:\")\n",
    "    print(panel['treated'].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"\\n⚠ Industry column not found - creating random treatment for demo\")\n",
    "    # Random assignment for demonstration (REPLACE WITH REAL DATA)\n",
    "    firm_treatment = df_wide[[FIRM_ID]].drop_duplicates()\n",
    "    firm_treatment['treated'] = np.random.binomial(1, 0.5, len(firm_treatment))\n",
    "    panel = panel.merge(firm_treatment, on=FIRM_ID, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# POST-TREATMENT INDICATOR\n",
    "# ============================================================================\n",
    "\n",
    "# ChatGPT released November 30, 2022\n",
    "# First full post-treatment quarter: Q1 2023\n",
    "# We code Q4 2022 as the \"event quarter\" (partially treated)\n",
    "\n",
    "CHATGPT_YEAR = 2022\n",
    "CHATGPT_QUARTER = 4\n",
    "\n",
    "# Post = 1 for Q1 2023 onwards (strictly post-release)\n",
    "panel['post'] = ((panel['year'] > CHATGPT_YEAR) | \n",
    "                 ((panel['year'] == CHATGPT_YEAR) & (panel['quarter'] > CHATGPT_QUARTER))).astype(int)\n",
    "\n",
    "# DiD interaction\n",
    "panel['treated_x_post'] = panel['treated'] * panel['post']\n",
    "\n",
    "# Event time (quarters relative to Q4 2022)\n",
    "event_yq = CHATGPT_YEAR * 4 + CHATGPT_QUARTER\n",
    "panel['event_time'] = panel['yearquarter'] - event_yq\n",
    "\n",
    "print(f\"Post-treatment observations: {panel['post'].sum():,} / {len(panel):,}\")\n",
    "print(f\"Event time range: [{panel['event_time'].min()}, {panel['event_time'].max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAMPLE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "# Keep only observations with:\n",
    "# 1. Non-missing outcome (SGA Efficiency)\n",
    "# 2. Non-missing treatment\n",
    "\n",
    "analysis_sample = panel[\n",
    "    (panel['sga_efficiency'].notna()) & \n",
    "    (panel['treated'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nAnalysis Sample:\")\n",
    "print(f\"  Observations: {len(analysis_sample):,}\")\n",
    "print(f\"  Firms: {analysis_sample[FIRM_ID].nunique():,}\")\n",
    "print(f\"  Periods: {analysis_sample['period'].nunique()}\")\n",
    "print(f\"  Treated firms: {analysis_sample[analysis_sample['treated']==1][FIRM_ID].nunique():,}\")\n",
    "print(f\"  Control firms: {analysis_sample[analysis_sample['treated']==0][FIRM_ID].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Descriptive Statistics & Parallel Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TABLE 1: SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def create_summary_table(df, outcome_vars, by_treatment=True):\n",
    "    \"\"\"\n",
    "    Create publication-ready summary statistics table.\n",
    "    \"\"\"\n",
    "    if by_treatment:\n",
    "        # Pre-period only for balance check\n",
    "        pre_data = df[df['post'] == 0]\n",
    "        \n",
    "        summary = []\n",
    "        for var in outcome_vars:\n",
    "            if var in pre_data.columns:\n",
    "                treated = pre_data[pre_data['treated'] == 1][var]\n",
    "                control = pre_data[pre_data['treated'] == 0][var]\n",
    "                \n",
    "                # T-test for difference\n",
    "                if len(treated.dropna()) > 0 and len(control.dropna()) > 0:\n",
    "                    tstat, pval = stats.ttest_ind(treated.dropna(), control.dropna())\n",
    "                else:\n",
    "                    tstat, pval = np.nan, np.nan\n",
    "                \n",
    "                summary.append({\n",
    "                    'Variable': var,\n",
    "                    'Treated Mean': treated.mean(),\n",
    "                    'Treated SD': treated.std(),\n",
    "                    'Control Mean': control.mean(),\n",
    "                    'Control SD': control.std(),\n",
    "                    'Difference': treated.mean() - control.mean(),\n",
    "                    't-stat': tstat,\n",
    "                    'p-value': pval\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(summary)\n",
    "    else:\n",
    "        return df[outcome_vars].describe().T\n",
    "\n",
    "# Define variables for summary\n",
    "summary_vars = ['sga_efficiency', 'revenue_per_employee', 'ebitda_margin', \n",
    "                'log_revenue', 'log_employees']\n",
    "summary_vars = [v for v in summary_vars if v in analysis_sample.columns]\n",
    "\n",
    "summary_table = create_summary_table(analysis_sample, summary_vars, by_treatment=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 1: SUMMARY STATISTICS (Pre-Period)\")\n",
    "print(\"=\" * 80)\n",
    "display(summary_table.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIGURE 1: PARALLEL TRENDS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_parallel_trends_publication(df, outcome, treatment_col='treated',\n",
    "                                      event_date='2022-11-01', save_path=None):\n",
    "    \"\"\"\n",
    "    Create publication-quality parallel trends figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Compute means by period and treatment\n",
    "    trends = df.groupby(['year', 'quarter', treatment_col])[outcome].mean().reset_index()\n",
    "    trends['date'] = pd.to_datetime(trends['year'].astype(str) + '-' + \n",
    "                                     ((trends['quarter']-1)*3 + 1).astype(str) + '-01')\n",
    "    \n",
    "    # Colors\n",
    "    colors = {1: '#2E86AB', 0: '#A23B72'}  # Blue for treated, Magenta for control\n",
    "    labels = {1: 'High AI Exposure (Treated)', 0: 'Low AI Exposure (Control)'}\n",
    "    \n",
    "    for treat_val in [0, 1]:\n",
    "        group = trends[trends[treatment_col] == treat_val].sort_values('date')\n",
    "        ax.plot(group['date'], group[outcome], \n",
    "                marker='o', markersize=6, linewidth=2,\n",
    "                color=colors[treat_val], label=labels[treat_val])\n",
    "    \n",
    "    # Event line\n",
    "    ax.axvline(pd.to_datetime(event_date), color='#E74C3C', linestyle='--', \n",
    "               linewidth=2, label='ChatGPT Release (Nov 2022)', alpha=0.8)\n",
    "    \n",
    "    # Shading for post-period\n",
    "    ax.axvspan(pd.to_datetime(event_date), trends['date'].max(), \n",
    "               alpha=0.1, color='gray', label='Post-Treatment Period')\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel(f'{outcome.replace(\"_\", \" \").title()}', fontsize=12)\n",
    "    ax.set_title('Figure 1: Parallel Trends in SG&A Efficiency', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.legend(loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotate x-labels\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot parallel trends\n",
    "if 'sga_efficiency' in analysis_sample.columns:\n",
    "    fig = plot_parallel_trends_publication(analysis_sample, 'sga_efficiency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Main DiD Specification\n",
    "\n",
    "**Specification:**\n",
    "$$Y_{it} = \\alpha_i + \\alpha_t + \\beta(Treated_i \\times Post_t) + \\epsilon_{it}$$\n",
    "\n",
    "Where:\n",
    "- $Y_{it}$ = SGA Efficiency (SG&A / Revenue)\n",
    "- $\\alpha_i$ = Firm fixed effects\n",
    "- $\\alpha_t$ = Year-Quarter fixed effects\n",
    "- $\\beta$ = **Treatment effect** (coefficient of interest)\n",
    "- Standard errors clustered at firm level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN DiD REGRESSION\n",
    "# ============================================================================\n",
    "\n",
    "def run_did_twfe(df, outcome, firm_id, time_var='period', \n",
    "                 treatment_interaction='treated_x_post', controls=None):\n",
    "    \"\"\"\n",
    "    Run Two-Way Fixed Effects DiD regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "    outcome : str, dependent variable\n",
    "    firm_id : str, firm identifier\n",
    "    time_var : str, time period identifier\n",
    "    treatment_interaction : str, DiD interaction term\n",
    "    controls : list, optional control variables\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    PanelOLS results object\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    keep_cols = [firm_id, time_var, outcome, treatment_interaction]\n",
    "    if controls:\n",
    "        keep_cols.extend(controls)\n",
    "    \n",
    "    reg_data = df[keep_cols].dropna().copy()\n",
    "    \n",
    "    if len(reg_data) < 100:\n",
    "        raise ValueError(f\"Insufficient observations: {len(reg_data)}\")\n",
    "    \n",
    "    # Set panel index\n",
    "    reg_data = reg_data.set_index([firm_id, time_var])\n",
    "    \n",
    "    # Define model\n",
    "    y = reg_data[outcome]\n",
    "    \n",
    "    X_cols = [treatment_interaction]\n",
    "    if controls:\n",
    "        X_cols.extend(controls)\n",
    "    \n",
    "    X = sm.add_constant(reg_data[X_cols])\n",
    "    \n",
    "    # Estimate with TWFE\n",
    "    model = PanelOLS(y, X, entity_effects=True, time_effects=True)\n",
    "    results = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run main regression\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 2: MAIN DiD RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "main_result = run_did_twfe(\n",
    "    analysis_sample, \n",
    "    outcome='sga_efficiency',\n",
    "    firm_id=FIRM_ID,\n",
    "    time_var='period',\n",
    "    treatment_interaction='treated_x_post'\n",
    ")\n",
    "\n",
    "print(main_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT KEY COEFFICIENT\n",
    "# ============================================================================\n",
    "\n",
    "beta_did = main_result.params['treated_x_post']\n",
    "se_did = main_result.std_errors['treated_x_post']\n",
    "tstat_did = main_result.tstats['treated_x_post']\n",
    "pval_did = main_result.pvalues['treated_x_post']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY RESULT: DiD Coefficient (β)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  β (Treated × Post) = {beta_did:.6f}\")\n",
    "print(f\"  Standard Error     = {se_did:.6f}\")\n",
    "print(f\"  t-statistic        = {tstat_did:.4f}\")\n",
    "print(f\"  p-value            = {pval_did:.6f}\")\n",
    "print(f\"\\n  Observations       = {main_result.nobs:,}\")\n",
    "print(f\"  R² (within)        = {main_result.rsquared_within:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Randomization Inference (5,000 Permutations)\n",
    "\n",
    "**Purpose:** Validate the DiD coefficient by showing it's unlikely to arise by chance.\n",
    "\n",
    "**Method:** \n",
    "1. Randomly reassign treatment across firms (keeping within-firm correlation)\n",
    "2. Re-estimate DiD 5,000 times\n",
    "3. Compute empirical p-value: proportion of fake coefficients more extreme than true coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RANDOMIZATION INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "def run_single_permutation(df, outcome, firm_id, time_var, seed):\n",
    "    \"\"\"\n",
    "    Run a single permutation of the DiD with shuffled treatment.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    try:\n",
    "        # Get firm-level treatment and shuffle\n",
    "        firm_treatment = df[[firm_id, 'treated']].drop_duplicates(subset=[firm_id])\n",
    "        shuffled_treatment = firm_treatment['treated'].values.copy()\n",
    "        np.random.shuffle(shuffled_treatment)\n",
    "        firm_treatment['treated_placebo'] = shuffled_treatment\n",
    "        \n",
    "        # Merge back\n",
    "        df_perm = df.drop(columns=['treated_x_post'], errors='ignore').merge(\n",
    "            firm_treatment[[firm_id, 'treated_placebo']], on=firm_id, how='left'\n",
    "        )\n",
    "        df_perm['treated_x_post_placebo'] = df_perm['treated_placebo'] * df_perm['post']\n",
    "        \n",
    "        # Prepare regression data\n",
    "        reg_data = df_perm[[firm_id, time_var, outcome, 'treated_x_post_placebo']].dropna()\n",
    "        reg_data = reg_data.set_index([firm_id, time_var])\n",
    "        \n",
    "        y = reg_data[outcome]\n",
    "        X = sm.add_constant(reg_data[['treated_x_post_placebo']])\n",
    "        \n",
    "        model = PanelOLS(y, X, entity_effects=True, time_effects=True)\n",
    "        result = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "        \n",
    "        return result.params['treated_x_post_placebo']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def run_randomization_inference(df, outcome, firm_id, time_var='period',\n",
    "                                 n_permutations=5000, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Run full randomization inference with parallel processing.\n",
    "    \"\"\"\n",
    "    print(f\"\\nRunning {n_permutations:,} permutations...\")\n",
    "    print(f\"Using {n_jobs} CPU cores (-1 = all available)\")\n",
    "    \n",
    "    # Generate seeds\n",
    "    seeds = np.random.randint(0, 1e7, n_permutations)\n",
    "    \n",
    "    # Run in parallel\n",
    "    placebo_coefs = Parallel(n_jobs=n_jobs, verbose=5)(\n",
    "        delayed(run_single_permutation)(df, outcome, firm_id, time_var, seed)\n",
    "        for seed in seeds\n",
    "    )\n",
    "    \n",
    "    return np.array(placebo_coefs)\n",
    "\n",
    "# Run randomization inference\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANDOMIZATION INFERENCE (5,000 Permutations)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "placebo_coefs = run_randomization_inference(\n",
    "    analysis_sample, \n",
    "    outcome='sga_efficiency',\n",
    "    firm_id=FIRM_ID,\n",
    "    time_var='period',\n",
    "    n_permutations=N_PERMUTATIONS,\n",
    "    n_jobs=N_CORES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPUTE EMPIRICAL P-VALUE\n",
    "# ============================================================================\n",
    "\n",
    "# Remove NaN values\n",
    "placebo_coefs_clean = placebo_coefs[~np.isnan(placebo_coefs)]\n",
    "\n",
    "print(f\"\\nSuccessful permutations: {len(placebo_coefs_clean):,} / {N_PERMUTATIONS:,}\")\n",
    "\n",
    "# Two-sided empirical p-value\n",
    "# P(|placebo| >= |true|)\n",
    "empirical_pval = np.mean(np.abs(placebo_coefs_clean) >= np.abs(beta_did))\n",
    "\n",
    "# One-sided (if we have directional hypothesis: beta < 0)\n",
    "empirical_pval_onesided = np.mean(placebo_coefs_clean <= beta_did)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RANDOMIZATION INFERENCE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  True DiD Coefficient (β):     {beta_did:.6f}\")\n",
    "print(f\"  Placebo Mean:                  {np.mean(placebo_coefs_clean):.6f}\")\n",
    "print(f\"  Placebo Std Dev:               {np.std(placebo_coefs_clean):.6f}\")\n",
    "print(f\"\\n  Empirical p-value (two-sided): {empirical_pval:.4f}\")\n",
    "print(f\"  Empirical p-value (one-sided): {empirical_pval_onesided:.4f}\")\n",
    "print(f\"\\n  Percentile of true β:          {percentileofscore(placebo_coefs_clean, beta_did):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIGURE 2: RANDOMIZATION INFERENCE HISTOGRAM\n",
    "# ============================================================================\n",
    "\n",
    "def plot_randomization_inference(placebo_coefs, true_coef, empirical_pval, save_path=None):\n",
    "    \"\"\"\n",
    "    Create publication-quality randomization inference figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Histogram of placebo coefficients\n",
    "    n, bins, patches = ax.hist(placebo_coefs, bins=80, density=True, \n",
    "                                alpha=0.7, color='#3498DB', edgecolor='white',\n",
    "                                label=f'Placebo Distribution (n={len(placebo_coefs):,})')\n",
    "    \n",
    "    # Kernel density estimate\n",
    "    from scipy.stats import gaussian_kde\n",
    "    kde = gaussian_kde(placebo_coefs[~np.isnan(placebo_coefs)])\n",
    "    x_range = np.linspace(placebo_coefs.min(), placebo_coefs.max(), 200)\n",
    "    ax.plot(x_range, kde(x_range), color='#2C3E50', linewidth=2, label='KDE')\n",
    "    \n",
    "    # True coefficient line\n",
    "    ax.axvline(true_coef, color='#E74C3C', linewidth=3, linestyle='--',\n",
    "               label=f'True β = {true_coef:.4f}')\n",
    "    \n",
    "    # Shade rejection region\n",
    "    rejection_threshold = np.percentile(placebo_coefs, [2.5, 97.5])\n",
    "    ax.axvline(rejection_threshold[0], color='gray', linewidth=1, linestyle=':')\n",
    "    ax.axvline(rejection_threshold[1], color='gray', linewidth=1, linestyle=':')\n",
    "    \n",
    "    # Add text annotations\n",
    "    ax.text(0.02, 0.98, f'Empirical p-value: {empirical_pval:.4f}',\n",
    "            transform=ax.transAxes, fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel('DiD Coefficient (β)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Figure 2: Randomization Inference\\nDistribution of Placebo Coefficients',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.legend(loc='upper right', frameon=True, fancybox=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot\n",
    "fig = plot_randomization_inference(placebo_coefs_clean, beta_did, empirical_pval)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Event Study\n",
    "\n",
    "Dynamic treatment effects to validate parallel trends and trace out the effect over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVENT STUDY SPECIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def run_event_study(df, outcome, firm_id, event_time_col='event_time',\n",
    "                    time_var='period', omit_period=-1,\n",
    "                    min_period=-12, max_period=8):\n",
    "    \"\"\"\n",
    "    Run event study regression with dynamic treatment effects.\n",
    "    \n",
    "    Y_it = α_i + α_t + Σ_k β_k (Treated_i × 1{t=k}) + ε_it\n",
    "    \n",
    "    Returns coefficient DataFrame for plotting.\n",
    "    \"\"\"\n",
    "    # Filter to event window\n",
    "    df_es = df[(df[event_time_col] >= min_period) & \n",
    "               (df[event_time_col] <= max_period)].copy()\n",
    "    \n",
    "    # Create event time dummies interacted with treatment\n",
    "    event_times = sorted(df_es[event_time_col].unique())\n",
    "    \n",
    "    for t in event_times:\n",
    "        if t != omit_period:\n",
    "            df_es[f'treat_t{t}'] = ((df_es[event_time_col] == t) * df_es['treated']).astype(float)\n",
    "    \n",
    "    # Regression\n",
    "    interact_cols = [c for c in df_es.columns if c.startswith('treat_t')]\n",
    "    \n",
    "    reg_data = df_es[[firm_id, time_var, outcome] + interact_cols].dropna()\n",
    "    reg_data = reg_data.set_index([firm_id, time_var])\n",
    "    \n",
    "    y = reg_data[outcome]\n",
    "    X = sm.add_constant(reg_data[interact_cols])\n",
    "    \n",
    "    model = PanelOLS(y, X, entity_effects=True, time_effects=True)\n",
    "    result = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "    \n",
    "    # Extract coefficients\n",
    "    coefs = []\n",
    "    for t in event_times:\n",
    "        if t == omit_period:\n",
    "            coefs.append({'event_time': t, 'coef': 0, 'se': 0, \n",
    "                         'ci_low': 0, 'ci_high': 0, 'pval': np.nan})\n",
    "        else:\n",
    "            col = f'treat_t{t}'\n",
    "            if col in result.params.index:\n",
    "                coef = result.params[col]\n",
    "                se = result.std_errors[col]\n",
    "                pval = result.pvalues[col]\n",
    "                coefs.append({\n",
    "                    'event_time': t,\n",
    "                    'coef': coef,\n",
    "                    'se': se,\n",
    "                    'ci_low': coef - 1.96 * se,\n",
    "                    'ci_high': coef + 1.96 * se,\n",
    "                    'pval': pval\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(coefs), result\n",
    "\n",
    "# Run event study\n",
    "es_coefs, es_result = run_event_study(\n",
    "    analysis_sample, \n",
    "    outcome='sga_efficiency',\n",
    "    firm_id=FIRM_ID,\n",
    "    event_time_col='event_time',\n",
    "    time_var='period'\n",
    ")\n",
    "\n",
    "print(\"\\nEvent Study Coefficients:\")\n",
    "display(es_coefs.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIGURE 3: EVENT STUDY PLOT\n",
    "# ============================================================================\n",
    "\n",
    "def plot_event_study_publication(coef_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create publication-quality event study figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Confidence intervals\n",
    "    ax.fill_between(coef_df['event_time'], coef_df['ci_low'], coef_df['ci_high'],\n",
    "                    alpha=0.25, color='#3498DB', label='95% CI')\n",
    "    \n",
    "    # Point estimates\n",
    "    ax.plot(coef_df['event_time'], coef_df['coef'], 'o-',\n",
    "            color='#2C3E50', linewidth=2.5, markersize=8, label='Point Estimate')\n",
    "    \n",
    "    # Reference lines\n",
    "    ax.axhline(0, color='black', linewidth=0.8, linestyle='-')\n",
    "    ax.axvline(0, color='#E74C3C', linewidth=2, linestyle='--',\n",
    "               label='ChatGPT Release (Q4 2022)')\n",
    "    \n",
    "    # Shade pre vs post\n",
    "    ax.axvspan(coef_df['event_time'].min(), 0, alpha=0.05, color='gray')\n",
    "    \n",
    "    ax.set_xlabel('Quarters Relative to ChatGPT Release', fontsize=12)\n",
    "    ax.set_ylabel('Effect on SG&A Efficiency (SG&A / Revenue)', fontsize=12)\n",
    "    ax.set_title('Figure 3: Event Study - Dynamic Treatment Effects\\n\"The Hollowing Out Effect\"',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.legend(loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Set x-ticks\n",
    "    ax.set_xticks(coef_df['event_time'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = plot_event_study_publication(es_coefs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Heterogeneity Analysis\n",
    "\n",
    "Testing whether the \"hollowing\" effect is stronger for certain types of firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HETEROGENEITY BY FIRM SIZE\n",
    "# ============================================================================\n",
    "\n",
    "def run_heterogeneity_analysis(df, outcome, firm_id, time_var='period',\n",
    "                                split_var='log_revenue', split_type='median'):\n",
    "    \"\"\"\n",
    "    Run DiD separately for subsamples (heterogeneity analysis).\n",
    "    \"\"\"\n",
    "    if split_var not in df.columns:\n",
    "        print(f\"Split variable {split_var} not found\")\n",
    "        return None\n",
    "    \n",
    "    # Compute split threshold (using pre-period values)\n",
    "    pre_period = df[df['post'] == 0]\n",
    "    firm_avg = pre_period.groupby(firm_id)[split_var].mean().reset_index()\n",
    "    \n",
    "    if split_type == 'median':\n",
    "        threshold = firm_avg[split_var].median()\n",
    "    elif split_type == 'tercile':\n",
    "        threshold = firm_avg[split_var].quantile([0.33, 0.67]).values\n",
    "    \n",
    "    firm_avg['size_group'] = (firm_avg[split_var] > threshold).map({True: 'Large', False: 'Small'})\n",
    "    \n",
    "    # Merge back\n",
    "    df_het = df.merge(firm_avg[[firm_id, 'size_group']], on=firm_id, how='left')\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for group in ['Large', 'Small']:\n",
    "        subset = df_het[df_het['size_group'] == group]\n",
    "        print(f\"\\n{group} firms: {subset[firm_id].nunique():,} firms, {len(subset):,} obs\")\n",
    "        \n",
    "        try:\n",
    "            result = run_did_twfe(subset, outcome, firm_id, time_var, 'treated_x_post')\n",
    "            results[group] = {\n",
    "                'coef': result.params['treated_x_post'],\n",
    "                'se': result.std_errors['treated_x_post'],\n",
    "                'pval': result.pvalues['treated_x_post'],\n",
    "                'nobs': result.nobs\n",
    "            }\n",
    "            print(f\"  β = {results[group]['coef']:.6f} (SE = {results[group]['se']:.6f}), p = {results[group]['pval']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            results[group] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run heterogeneity analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 3: HETEROGENEITY BY FIRM SIZE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "het_results = run_heterogeneity_analysis(\n",
    "    analysis_sample,\n",
    "    outcome='sga_efficiency',\n",
    "    firm_id=FIRM_ID,\n",
    "    time_var='period',\n",
    "    split_var='log_revenue' if 'log_revenue' in analysis_sample.columns else 'log_employees'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRIPLE DIFFERENCE (if we have industry variation)\n",
    "# ============================================================================\n",
    "\n",
    "def run_triple_difference(df, outcome, firm_id, time_var='period',\n",
    "                          moderator='log_revenue'):\n",
    "    \"\"\"\n",
    "    Run triple-difference specification:\n",
    "    Y_it = α_i + α_t + β1(T×Post) + β2(T×Post×Moderator) + ε_it\n",
    "    \"\"\"\n",
    "    if moderator not in df.columns:\n",
    "        print(f\"Moderator {moderator} not found\")\n",
    "        return None\n",
    "    \n",
    "    df_ddd = df.copy()\n",
    "    \n",
    "    # Standardize moderator\n",
    "    df_ddd['mod_std'] = (df_ddd[moderator] - df_ddd[moderator].mean()) / df_ddd[moderator].std()\n",
    "    \n",
    "    # Triple interaction\n",
    "    df_ddd['triple_interact'] = df_ddd['treated_x_post'] * df_ddd['mod_std']\n",
    "    \n",
    "    # Regression\n",
    "    reg_data = df_ddd[[firm_id, time_var, outcome, 'treated_x_post', 'triple_interact']].dropna()\n",
    "    reg_data = reg_data.set_index([firm_id, time_var])\n",
    "    \n",
    "    y = reg_data[outcome]\n",
    "    X = sm.add_constant(reg_data[['treated_x_post', 'triple_interact']])\n",
    "    \n",
    "    model = PanelOLS(y, X, entity_effects=True, time_effects=True)\n",
    "    result = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run triple-diff if we have size variable\n",
    "if 'log_revenue' in analysis_sample.columns or 'log_employees' in analysis_sample.columns:\n",
    "    mod_var = 'log_revenue' if 'log_revenue' in analysis_sample.columns else 'log_employees'\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"TRIPLE DIFFERENCE (Moderator: {mod_var})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ddd_result = run_triple_difference(\n",
    "        analysis_sample, 'sga_efficiency', FIRM_ID, 'period', mod_var\n",
    "    )\n",
    "    \n",
    "    if ddd_result:\n",
    "        print(ddd_result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Additional Outcomes (Multiple Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTIPLE OUTCOMES\n",
    "# ============================================================================\n",
    "\n",
    "outcomes_to_test = [\n",
    "    ('sga_efficiency', 'SG&A / Revenue'),\n",
    "    ('revenue_per_employee', 'Revenue / Employee'),\n",
    "    ('ebitda_margin', 'EBITDA / Revenue'),\n",
    "]\n",
    "\n",
    "# Filter to available outcomes\n",
    "outcomes_to_test = [(var, label) for var, label in outcomes_to_test \n",
    "                    if var in analysis_sample.columns]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE 4: MULTIPLE OUTCOME ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "multi_results = []\n",
    "\n",
    "for var, label in outcomes_to_test:\n",
    "    try:\n",
    "        result = run_did_twfe(analysis_sample, var, FIRM_ID, 'period', 'treated_x_post')\n",
    "        multi_results.append({\n",
    "            'Outcome': label,\n",
    "            'β (Treated × Post)': result.params['treated_x_post'],\n",
    "            'Std. Error': result.std_errors['treated_x_post'],\n",
    "            't-stat': result.tstats['treated_x_post'],\n",
    "            'p-value': result.pvalues['treated_x_post'],\n",
    "            'N': result.nobs,\n",
    "            'R² (within)': result.rsquared_within\n",
    "        })\n",
    "        print(f\"\\n{label}:\")\n",
    "        print(f\"  β = {result.params['treated_x_post']:.6f}, p = {result.pvalues['treated_x_post']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{label}: Error - {e}\")\n",
    "\n",
    "if multi_results:\n",
    "    multi_df = pd.DataFrame(multi_results)\n",
    "    print(\"\\n\")\n",
    "    display(multi_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Causal Forest (Heterogeneous Treatment Effects)\n",
    "\n",
    "Using machine learning to discover which firm characteristics predict stronger treatment effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CAUSAL FOREST (requires EconML)\n",
    "# ============================================================================\n",
    "\n",
    "if ECONML_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CAUSAL FOREST: HETEROGENEOUS TREATMENT EFFECTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Prepare data for causal forest\n",
    "    # Use post-period data only, predict individual treatment effects\n",
    "    \n",
    "    # Find available covariates\n",
    "    potential_covariates = ['log_revenue', 'log_employees', 'log_assets']\n",
    "    covariates = [c for c in potential_covariates if c in analysis_sample.columns]\n",
    "    \n",
    "    if len(covariates) >= 2:\n",
    "        cf_data = analysis_sample[\n",
    "            analysis_sample['post'] == 1\n",
    "        ][['sga_efficiency', 'treated'] + covariates].dropna()\n",
    "        \n",
    "        Y = cf_data['sga_efficiency'].values\n",
    "        T = cf_data['treated'].values\n",
    "        X = cf_data[covariates].values\n",
    "        \n",
    "        print(f\"\\nCausal Forest sample: {len(cf_data):,} observations\")\n",
    "        print(f\"Covariates: {covariates}\")\n",
    "        \n",
    "        try:\n",
    "            # Fit causal forest\n",
    "            cf = CausalForestDML(\n",
    "                model_y=GradientBoostingRegressor(n_estimators=100, max_depth=4),\n",
    "                model_t=GradientBoostingRegressor(n_estimators=100, max_depth=4),\n",
    "                n_estimators=200,\n",
    "                min_samples_leaf=20,\n",
    "                random_state=RANDOM_SEED,\n",
    "                n_jobs=N_CORES\n",
    "            )\n",
    "            \n",
    "            cf.fit(Y, T, X=X)\n",
    "            \n",
    "            # Get treatment effects\n",
    "            treatment_effects = cf.effect(X)\n",
    "            \n",
    "            print(f\"\\nAverage Treatment Effect (ATE): {treatment_effects.mean():.6f}\")\n",
    "            print(f\"Treatment Effect Std Dev: {treatment_effects.std():.6f}\")\n",
    "            print(f\"Treatment Effect Range: [{treatment_effects.min():.6f}, {treatment_effects.max():.6f}]\")\n",
    "            \n",
    "            # Feature importance\n",
    "            print(\"\\nFeature Importance for Treatment Effect Heterogeneity:\")\n",
    "            importance = cf.feature_importances_\n",
    "            for cov, imp in zip(covariates, importance):\n",
    "                print(f\"  {cov}: {imp:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Causal Forest error: {e}\")\n",
    "    else:\n",
    "        print(\"Insufficient covariates for Causal Forest\")\n",
    "else:\n",
    "    print(\"\\nCausal Forest skipped - EconML not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Save Results and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE ALL RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Save analysis panel\n",
    "analysis_sample.to_parquet(DATA_PATH / 'analysis_panel_hollow_firm.parquet', index=False)\n",
    "\n",
    "# Save event study coefficients\n",
    "es_coefs.to_csv(DATA_PATH / 'event_study_coefficients.csv', index=False)\n",
    "\n",
    "# Save placebo distribution\n",
    "np.save(DATA_PATH / 'placebo_coefficients.npy', placebo_coefs_clean)\n",
    "\n",
    "# Save summary results\n",
    "results_summary = {\n",
    "    'main_coefficient': beta_did,\n",
    "    'main_se': se_did,\n",
    "    'main_pvalue': pval_did,\n",
    "    'empirical_pvalue': empirical_pval,\n",
    "    'n_permutations': len(placebo_coefs_clean),\n",
    "    'n_observations': main_result.nobs,\n",
    "    'n_firms': analysis_sample[FIRM_ID].nunique(),\n",
    "    'r_squared_within': main_result.rsquared_within\n",
    "}\n",
    "\n",
    "pd.Series(results_summary).to_csv(DATA_PATH / 'results_summary.csv')\n",
    "\n",
    "print(\"\\n✓ All results saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE: THE HOLLOW FIRM HYPOTHESIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           MAIN RESULT                                       │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│  DiD Coefficient (β):           {beta_did:>12.6f}                               │\n",
    "│  Standard Error:                {se_did:>12.6f}                               │\n",
    "│  Conventional p-value:          {pval_did:>12.6f}                               │\n",
    "│  Randomization p-value:         {empirical_pval:>12.6f}                               │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│  Observations:                  {main_result.nobs:>12,}                               │\n",
    "│  Unique Firms:                  {analysis_sample[FIRM_ID].nunique():>12,}                               │\n",
    "│  R² (within):                   {main_result.rsquared_within:>12.4f}                               │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Professor's Commentary\n",
    "\n",
    "### Interpretation of a Negative, Significant β\n",
    "\n",
    "If $\\beta < 0$ and statistically significant, here's how to interpret:\n",
    "\n",
    "**Plain English:**\n",
    "> \"Following the release of ChatGPT, firms with high AI exposure reduced their SG&A-to-Revenue ratio by [β × 100] percentage points more than firms with low AI exposure, controlling for firm-specific factors and aggregate time trends.\"\n",
    "\n",
    "**Economic Magnitude:**\n",
    "- If β = -0.02, this means a 2 percentage point reduction in SG&A/Revenue\n",
    "- For a firm with $1B revenue, this represents $20M in reduced overhead costs\n",
    "- Relative to pre-period mean SG&A efficiency of ~25%, this is an 8% reduction\n",
    "\n",
    "**Causal Claim:**\n",
    "The DiD design with firm and time fixed effects, combined with:\n",
    "1. Parallel pre-trends (visible in event study)\n",
    "2. Sharp post-treatment break\n",
    "3. Randomization inference validation\n",
    "\n",
    "...supports a **causal interpretation**: GenAI *caused* high-exposure firms to become more organizationally efficient (\"hollow\").\n",
    "\n",
    "**Mechanism:**\n",
    "The \"hollowing\" likely reflects:\n",
    "- Automation of middle-management tasks (reporting, coordination)\n",
    "- Reduced administrative overhead (HR, legal, compliance assistance)\n",
    "- Streamlined customer service and support functions\n",
    "\n",
    "**Publication-Worthiness:**\n",
    "This result is tier-1 worthy because:\n",
    "1. **Novel mechanism**: Not just \"AI replaces workers\" but \"AI replaces *organizational friction*\"\n",
    "2. **Clean identification**: ChatGPT is a sharp, unexpected shock\n",
    "3. **Robust inference**: Both conventional and randomization p-values support significance\n",
    "4. **Economic significance**: The magnitude matters for corporate strategy and labor policy\n",
    "\n",
    "---\n",
    "\n",
    "### Caveats to Address in Paper\n",
    "\n",
    "1. **Short post-period**: Only ~2 years of post-data; effects may evolve\n",
    "2. **Treatment measurement**: Industry-level exposure may miss within-industry variation\n",
    "3. **Confounders**: Fed rate hikes (2022-23) may differentially affect treated firms\n",
    "4. **Anticipation**: Some firms may have anticipated AI impact before ChatGPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
