{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Dataset Exploration & Research Feasibility Analysis\n",
    "\n",
    "**Objective:** Profile the dataset and determine which tier-1 research questions are feasible.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "DATA_PATH = Path('/content/drive/MyDrive/Paper_2')\n",
    "\n",
    "# List files\n",
    "print(\"Files in data directory:\")\n",
    "for f in DATA_PATH.iterdir():\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both Excel files\n",
    "df1 = pd.read_excel(DATA_PATH / 'Data_1.xlsx')\n",
    "df2 = pd.read_excel(DATA_PATH / 'Data_2.xlsx')\n",
    "\n",
    "print(f\"Data_1.xlsx shape: {df1.shape}\")\n",
    "print(f\"Data_2.xlsx shape: {df2.shape}\")\n",
    "print(f\"\\nTotal columns across both files: {df1.shape[1] + df2.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Data_1\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA_1.xlsx - First 5 rows\")\n",
    "print(\"=\" * 80)\n",
    "display(df1.head())\n",
    "print(f\"\\nColumns ({len(df1.columns)}):\")\n",
    "print(df1.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Data_2\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA_2.xlsx - First 5 rows\")\n",
    "print(\"=\" * 80)\n",
    "display(df2.head())\n",
    "print(f\"\\nColumns ({len(df2.columns)}):\")\n",
    "print(df2.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Column Classification\n",
    "\n",
    "Based on the data dictionary provided:\n",
    "- **Identifiers (1-5):** Company Name, Exchange:Ticker, Geographic Locations, Company Type, Industry Classifications\n",
    "- **Market Cap:** 10 annual snapshots (Latest to Latest-9 Years)\n",
    "- **Profitability:** Gross Profit, Operating Income, EBITDA, Net Income (10 quarterly LTM)\n",
    "- **Expenses:** R&D, Interest Expense (10 quarterly LTM)\n",
    "- **Balance Sheet:** Cash, Receivables, Inventory, Current Assets, PP&E, Total Assets, Debt (10 quarterly)\n",
    "- **Cash Flow:** CapEx, Acquisitions (10 quarterly LTM)\n",
    "- **Ratios:** ROA%, ROE%, Debt/Equity (10 quarterly)\n",
    "- **Other:** Employees, Revenue (10 quarterly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_columns(columns):\n",
    "    \"\"\"Classify columns by type based on naming patterns.\"\"\"\n",
    "    classification = {\n",
    "        'identifiers': [],\n",
    "        'market_cap': [],\n",
    "        'profitability': [],\n",
    "        'expenses': [],\n",
    "        'balance_sheet': [],\n",
    "        'cash_flow': [],\n",
    "        'ratios': [],\n",
    "        'other_metrics': [],\n",
    "        'unknown': []\n",
    "    }\n",
    "    \n",
    "    identifier_keywords = ['name', 'ticker', 'exchange', 'location', 'type', 'industry', 'sector', 'country']\n",
    "    market_cap_keywords = ['market cap', 'marketcap', 'market_cap', 'mkt cap']\n",
    "    profitability_keywords = ['gross profit', 'operating income', 'ebitda', 'net income', 'ebit']\n",
    "    expense_keywords = ['r&d', 'research', 'interest expense', 'sga', 'selling']\n",
    "    balance_sheet_keywords = ['cash', 'receivable', 'inventory', 'current asset', 'ppe', 'pp&e', \n",
    "                              'total asset', 'current debt', 'long-term debt', 'lt debt', 'total debt']\n",
    "    cash_flow_keywords = ['capex', 'capital expenditure', 'acquisition', 'fcf', 'free cash']\n",
    "    ratio_keywords = ['roa', 'roe', 'debt/equity', 'debt to equity', 'margin', 'ratio']\n",
    "    other_keywords = ['employee', 'revenue', 'sales', 'headcount']\n",
    "    \n",
    "    for col in columns:\n",
    "        col_lower = col.lower()\n",
    "        \n",
    "        if any(kw in col_lower for kw in identifier_keywords):\n",
    "            classification['identifiers'].append(col)\n",
    "        elif any(kw in col_lower for kw in market_cap_keywords):\n",
    "            classification['market_cap'].append(col)\n",
    "        elif any(kw in col_lower for kw in profitability_keywords):\n",
    "            classification['profitability'].append(col)\n",
    "        elif any(kw in col_lower for kw in expense_keywords):\n",
    "            classification['expenses'].append(col)\n",
    "        elif any(kw in col_lower for kw in balance_sheet_keywords):\n",
    "            classification['balance_sheet'].append(col)\n",
    "        elif any(kw in col_lower for kw in cash_flow_keywords):\n",
    "            classification['cash_flow'].append(col)\n",
    "        elif any(kw in col_lower for kw in ratio_keywords):\n",
    "            classification['ratios'].append(col)\n",
    "        elif any(kw in col_lower for kw in other_keywords):\n",
    "            classification['other_metrics'].append(col)\n",
    "        else:\n",
    "            classification['unknown'].append(col)\n",
    "    \n",
    "    return classification\n",
    "\n",
    "# Classify columns from both dataframes\n",
    "all_columns = list(df1.columns) + list(df2.columns)\n",
    "classified = classify_columns(all_columns)\n",
    "\n",
    "print(\"Column Classification Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for category, cols in classified.items():\n",
    "    print(f\"\\n{category.upper()} ({len(cols)} columns):\")\n",
    "    if cols:\n",
    "        for col in cols[:5]:  # Show first 5\n",
    "            print(f\"  - {col}\")\n",
    "        if len(cols) > 5:\n",
    "            print(f\"  ... and {len(cols) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Period Identification\n",
    "\n",
    "**Critical for research design:** We need to identify the exact time coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_info(columns):\n",
    "    \"\"\"Extract time period information from column names.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    time_patterns = {\n",
    "        'annual': r'Latest\\s*-?\\s*(\\d+)\\s*Year',\n",
    "        'ltm': r'LTM\\s*-?\\s*(\\d+)',\n",
    "        'quarterly': r'Latest\\s*Quarter\\s*-?\\s*(\\d+)',\n",
    "        'latest': r'^.*Latest(?!.*-).*$'\n",
    "    }\n",
    "    \n",
    "    time_info = {'annual_range': [], 'ltm_range': [], 'quarterly_range': []}\n",
    "    \n",
    "    for col in columns:\n",
    "        # Check for annual\n",
    "        match = re.search(time_patterns['annual'], col, re.IGNORECASE)\n",
    "        if match:\n",
    "            time_info['annual_range'].append(int(match.group(1)))\n",
    "        \n",
    "        # Check for LTM\n",
    "        match = re.search(time_patterns['ltm'], col, re.IGNORECASE)\n",
    "        if match:\n",
    "            time_info['ltm_range'].append(int(match.group(1)))\n",
    "        \n",
    "        # Check for quarterly\n",
    "        match = re.search(time_patterns['quarterly'], col, re.IGNORECASE)\n",
    "        if match:\n",
    "            time_info['quarterly_range'].append(int(match.group(1)))\n",
    "    \n",
    "    return time_info\n",
    "\n",
    "time_info = extract_time_info(all_columns)\n",
    "\n",
    "print(\"Time Coverage Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "if time_info['annual_range']:\n",
    "    print(f\"Annual data: Latest to Latest - {max(time_info['annual_range'])} Years\")\n",
    "if time_info['ltm_range']:\n",
    "    print(f\"LTM data: LTM to LTM - {max(time_info['ltm_range'])} quarters ({max(time_info['ltm_range'])//4} years)\")\n",
    "if time_info['quarterly_range']:\n",
    "    print(f\"Quarterly data: Latest Quarter to Latest Quarter - {max(time_info['quarterly_range'])} ({max(time_info['quarterly_range'])//4} years)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Size & Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common identifier to merge\n",
    "print(\"Looking for merge keys between Data_1 and Data_2...\")\n",
    "common_cols = set(df1.columns) & set(df2.columns)\n",
    "print(f\"Common columns: {common_cols}\")\n",
    "\n",
    "# If there are common columns, try to merge\n",
    "if common_cols:\n",
    "    merge_key = list(common_cols)[0]\n",
    "    print(f\"\\nMerging on: {merge_key}\")\n",
    "    df = pd.merge(df1, df2, on=list(common_cols), how='outer')\n",
    "    print(f\"Merged dataset shape: {df.shape}\")\n",
    "else:\n",
    "    # Check if they have same number of rows (might be column split)\n",
    "    if len(df1) == len(df2):\n",
    "        print(\"\\nSame number of rows - concatenating horizontally\")\n",
    "        df = pd.concat([df1, df2], axis=1)\n",
    "        print(f\"Combined dataset shape: {df.shape}\")\n",
    "    else:\n",
    "        print(\"\\nDifferent row counts - keeping separate for now\")\n",
    "        df = df1  # Use df1 as primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of companies: {len(df)}\")\n",
    "print(f\"Number of variables: {len(df.columns)}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "def missing_analysis(dataframe):\n",
    "    \"\"\"Analyze missing values in the dataset.\"\"\"\n",
    "    missing = dataframe.isnull().sum()\n",
    "    missing_pct = (missing / len(dataframe)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing.index,\n",
    "        'Missing Count': missing.values,\n",
    "        'Missing %': missing_pct.values\n",
    "    })\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
    "    missing_df = missing_df.sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "missing_df = missing_analysis(df)\n",
    "print(f\"\\nColumns with missing values: {len(missing_df)} out of {len(df.columns)}\")\n",
    "print(\"\\nTop 20 columns by missing %:\")\n",
    "display(missing_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data pattern\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Sample columns for visualization (every 10th column)\n",
    "sample_cols = df.columns[::max(1, len(df.columns)//30)]\n",
    "missing_pct = df[sample_cols].isnull().mean() * 100\n",
    "\n",
    "missing_pct.plot(kind='bar', ax=ax, color='steelblue', alpha=0.7)\n",
    "ax.set_ylabel('Missing %')\n",
    "ax.set_title('Missing Data Pattern (Sampled Columns)')\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Industry & Geographic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find identifier columns\n",
    "id_cols = classified['identifiers']\n",
    "print(f\"Identifier columns found: {id_cols}\")\n",
    "\n",
    "# Try to identify industry and geography columns\n",
    "for col in id_cols:\n",
    "    print(f\"\\n{col} - Unique values: {df[col].nunique()}\")\n",
    "    if df[col].nunique() <= 50:  # Show distribution if reasonable\n",
    "        print(df[col].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Industry distribution (adjust column name as needed)\n",
    "industry_col = None\n",
    "for col in df.columns:\n",
    "    if 'industry' in col.lower() or 'sector' in col.lower():\n",
    "        industry_col = col\n",
    "        break\n",
    "\n",
    "if industry_col:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    industry_counts = df[industry_col].value_counts().head(20)\n",
    "    industry_counts.plot(kind='barh', ax=ax, color='steelblue')\n",
    "    ax.set_xlabel('Number of Companies')\n",
    "    ax.set_title(f'Top 20 Industries ({industry_col})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Industry column not found - please identify manually from column list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Metrics Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest market cap column\n",
    "mktcap_col = None\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if ('market cap' in col_lower or 'marketcap' in col_lower) and 'latest' in col_lower:\n",
    "        if '-' not in col_lower or 'latest]' in col_lower:\n",
    "            mktcap_col = col\n",
    "            break\n",
    "\n",
    "# Find latest revenue column\n",
    "revenue_col = None\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'revenue' in col_lower and ('ltm]' in col_lower or 'latest' in col_lower):\n",
    "        if '-' not in col_lower:\n",
    "            revenue_col = col\n",
    "            break\n",
    "\n",
    "# Find employee column\n",
    "emp_col = None\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'employee' in col_lower and ('ltm]' in col_lower or 'latest' in col_lower):\n",
    "        if '-' not in col_lower:\n",
    "            emp_col = col\n",
    "            break\n",
    "\n",
    "print(f\"Market Cap column: {mktcap_col}\")\n",
    "print(f\"Revenue column: {revenue_col}\")\n",
    "print(f\"Employee column: {emp_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key metrics\n",
    "metrics_to_plot = [col for col in [mktcap_col, revenue_col, emp_col] if col is not None]\n",
    "\n",
    "if metrics_to_plot:\n",
    "    fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(5*len(metrics_to_plot), 4))\n",
    "    if len(metrics_to_plot) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, col in zip(axes, metrics_to_plot):\n",
    "        data = df[col].dropna()\n",
    "        # Use log scale for financial data\n",
    "        data_positive = data[data > 0]\n",
    "        if len(data_positive) > 0:\n",
    "            np.log10(data_positive).hist(bins=50, ax=ax, color='steelblue', alpha=0.7)\n",
    "            ax.set_xlabel(f'Log10({col[:30]}...)')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'Distribution (n={len(data_positive):,})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for key metrics\n",
    "if metrics_to_plot:\n",
    "    print(\"\\nSummary Statistics (in USD millions, except employees):\")\n",
    "    print(\"=\" * 80)\n",
    "    summary = df[metrics_to_plot].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99])\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Research Feasibility Assessment\n",
    "\n",
    "Based on the data exploration above, let's assess which research ideas are feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key dates for research design\n",
    "KEY_EVENTS = {\n",
    "    'ChatGPT Release': '2022-11-30',\n",
    "    'COVID Pandemic': '2020-03-11',\n",
    "    'Fed Rate Hikes Begin': '2022-03-16',\n",
    "    'US-China Trade War Escalation': '2018-07-06',\n",
    "    'SVB Collapse': '2023-03-10'\n",
    "}\n",
    "\n",
    "print(\"Key Events for Natural Experiments:\")\n",
    "print(\"=\" * 60)\n",
    "for event, date in KEY_EVENTS.items():\n",
    "    print(f\"{event}: {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_research_feasibility(df, classified_cols):\n",
    "    \"\"\"\n",
    "    Assess feasibility of different research questions based on data availability.\n",
    "    \"\"\"\n",
    "    assessment = {}\n",
    "    \n",
    "    # 1. GenAI & Corporate Policy\n",
    "    # Requires: R&D, CapEx, Employees, Market Cap, Industry classification\n",
    "    genai_vars = ['expenses', 'cash_flow', 'other_metrics', 'market_cap', 'identifiers']\n",
    "    genai_available = sum(1 for v in genai_vars if len(classified_cols.get(v, [])) > 0)\n",
    "    assessment['GenAI & Corporate Policy'] = {\n",
    "        'feasibility': 'HIGH' if genai_available >= 4 else 'MEDIUM' if genai_available >= 3 else 'LOW',\n",
    "        'required_vars': genai_vars,\n",
    "        'available': genai_available,\n",
    "        'notes': 'Need R&D expense, CapEx, employee count, and industry to measure AI exposure'\n",
    "    }\n",
    "    \n",
    "    # 2. COVID & Capital Structure\n",
    "    # Requires: Debt, Cash, Total Assets, Profitability\n",
    "    covid_vars = ['balance_sheet', 'profitability', 'ratios']\n",
    "    covid_available = sum(1 for v in covid_vars if len(classified_cols.get(v, [])) > 0)\n",
    "    assessment['COVID & Capital Structure'] = {\n",
    "        'feasibility': 'HIGH' if covid_available == 3 else 'MEDIUM' if covid_available >= 2 else 'LOW',\n",
    "        'required_vars': covid_vars,\n",
    "        'available': covid_available,\n",
    "        'notes': 'Strong data for leverage, liquidity, and profitability analysis'\n",
    "    }\n",
    "    \n",
    "    # 3. Interest Rate Shock\n",
    "    # Requires: Debt, Interest Expense, Investment (CapEx)\n",
    "    rate_vars = ['balance_sheet', 'expenses', 'cash_flow']\n",
    "    rate_available = sum(1 for v in rate_vars if len(classified_cols.get(v, [])) > 0)\n",
    "    assessment['Interest Rate Shock (2022)'] = {\n",
    "        'feasibility': 'HIGH' if rate_available == 3 else 'MEDIUM' if rate_available >= 2 else 'LOW',\n",
    "        'required_vars': rate_vars,\n",
    "        'available': rate_available,\n",
    "        'notes': 'Interest expense and debt structure are key; can measure real effects via CapEx'\n",
    "    }\n",
    "    \n",
    "    # 4. Labor vs Capital (Automation)\n",
    "    # Requires: Employees, CapEx, Revenue, Industry\n",
    "    labor_vars = ['other_metrics', 'cash_flow', 'identifiers']\n",
    "    labor_available = sum(1 for v in labor_vars if len(classified_cols.get(v, [])) > 0)\n",
    "    assessment['Labor-Capital Substitution'] = {\n",
    "        'feasibility': 'HIGH' if labor_available == 3 else 'MEDIUM' if labor_available >= 2 else 'LOW',\n",
    "        'required_vars': labor_vars,\n",
    "        'available': labor_available,\n",
    "        'notes': 'Employee count + CapEx + Revenue allows productivity analysis'\n",
    "    }\n",
    "    \n",
    "    return assessment\n",
    "\n",
    "feasibility = assess_research_feasibility(df, classified)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESEARCH FEASIBILITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idea, details in feasibility.items():\n",
    "    print(f\"\\n{idea}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Feasibility: {details['feasibility']}\")\n",
    "    print(f\"  Data coverage: {details['available']}/{len(details['required_vars'])} variable groups\")\n",
    "    print(f\"  Notes: {details['notes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Checks for Panel Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate companies\n",
    "name_col = None\n",
    "ticker_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'name' in col_lower and name_col is None:\n",
    "        name_col = col\n",
    "    if 'ticker' in col_lower and ticker_col is None:\n",
    "        ticker_col = col\n",
    "\n",
    "if ticker_col:\n",
    "    duplicates = df[ticker_col].duplicated().sum()\n",
    "    print(f\"Duplicate tickers: {duplicates}\")\n",
    "    if duplicates > 0:\n",
    "        print(\"Duplicate examples:\")\n",
    "        dup_tickers = df[df[ticker_col].duplicated(keep=False)][ticker_col].unique()[:5]\n",
    "        print(dup_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check time series completeness for a sample metric\n",
    "# Look for all columns related to a single metric across time\n",
    "\n",
    "def check_time_series_completeness(df, metric_keyword):\n",
    "    \"\"\"Check how complete the time series is for a given metric.\"\"\"\n",
    "    cols = [c for c in df.columns if metric_keyword.lower() in c.lower()]\n",
    "    \n",
    "    if not cols:\n",
    "        return None\n",
    "    \n",
    "    completeness = df[cols].notna().mean() * 100\n",
    "    return completeness.sort_values(ascending=False)\n",
    "\n",
    "# Check completeness for key metrics\n",
    "for metric in ['revenue', 'market cap', 'employee', 'ebitda']:\n",
    "    completeness = check_time_series_completeness(df, metric)\n",
    "    if completeness is not None:\n",
    "        print(f\"\\n{metric.upper()} - Time Series Completeness:\")\n",
    "        print(completeness.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Processed Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data dictionary\n",
    "data_dict = []\n",
    "\n",
    "for col in df.columns:\n",
    "    data_dict.append({\n",
    "        'Column Name': col,\n",
    "        'Data Type': str(df[col].dtype),\n",
    "        'Non-Null Count': df[col].notna().sum(),\n",
    "        'Null %': (df[col].isna().sum() / len(df)) * 100,\n",
    "        'Unique Values': df[col].nunique(),\n",
    "        'Sample Value': df[col].dropna().iloc[0] if df[col].notna().any() else None\n",
    "    })\n",
    "\n",
    "data_dict_df = pd.DataFrame(data_dict)\n",
    "print(f\"Data dictionary created with {len(data_dict_df)} columns\")\n",
    "display(data_dict_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data dictionary\n",
    "data_dict_df.to_csv(DATA_PATH / 'data_dictionary.csv', index=False)\n",
    "print(f\"Data dictionary saved to {DATA_PATH / 'data_dictionary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps Summary\n",
    "\n",
    "Based on this exploration, please review:\n",
    "\n",
    "1. **Time coverage**: Does \"Latest\" refer to 2024? This determines which events you can study.\n",
    "\n",
    "2. **Sample size**: Is the number of companies sufficient for your research design?\n",
    "\n",
    "3. **Industry coverage**: Do you have enough variation in AI-exposed vs. non-exposed industries?\n",
    "\n",
    "4. **Missing data patterns**: Are key variables available across your event window?\n",
    "\n",
    "Run this notebook and share the outputs so we can proceed with the specific research design!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPLORATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal companies: {len(df):,}\")\n",
    "print(f\"Total variables: {len(df.columns):,}\")\n",
    "print(f\"\\nPlease share the output of this notebook to proceed with research design.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
