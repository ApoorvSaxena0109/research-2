{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing AI Exposure Treatment Variable\n",
    "\n",
    "This notebook builds the firm-level AI exposure measure following Felten, Raj, & Seamans (2023).\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. Load occupation-level AI exposure scores (from academic literature)\n",
    "2. Map industries to occupational composition (using BLS data)\n",
    "3. Compute industry-level AI exposure as weighted average\n",
    "4. Merge to firm-level data based on industry classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "DATA_PATH = Path('/content/drive/MyDrive/Paper_2')\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Your Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (adjust based on exploration notebook findings)\n",
    "df1 = pd.read_excel(DATA_PATH / 'Data_1.xlsx')\n",
    "df2 = pd.read_excel(DATA_PATH / 'Data_2.xlsx')\n",
    "\n",
    "# Determine how to combine (update based on notebook 01 results)\n",
    "common_cols = set(df1.columns) & set(df2.columns)\n",
    "if common_cols:\n",
    "    df = pd.merge(df1, df2, on=list(common_cols), how='outer')\n",
    "elif len(df1) == len(df2):\n",
    "    df = pd.concat([df1, df2], axis=1)\n",
    "else:\n",
    "    df = df1\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few columns: {df.columns[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify industry column\n",
    "# UPDATE THIS BASED ON YOUR ACTUAL COLUMN NAME\n",
    "industry_col = None\n",
    "for col in df.columns:\n",
    "    if 'industry' in col.lower() or 'sector' in col.lower() or 'sic' in col.lower() or 'naics' in col.lower():\n",
    "        print(f\"Found potential industry column: {col}\")\n",
    "        print(f\"  Unique values: {df[col].nunique()}\")\n",
    "        print(f\"  Sample: {df[col].dropna().head(5).tolist()}\")\n",
    "        industry_col = col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AI Exposure Data\n",
    "\n",
    "We use the AI exposure scores from academic research. Two main approaches:\n",
    "\n",
    "### Option A: Felten et al. (2023) - \"AI and Workforce\"\n",
    "- Measures how applicable AI capabilities are to each occupation\n",
    "- Based on O*NET ability requirements\n",
    "\n",
    "### Option B: Webb (2020) - Patent-based exposure\n",
    "- Measures overlap between AI patents and occupational tasks\n",
    "\n",
    "### Option C: Simple Industry Classification\n",
    "- Use commonly accepted high-AI vs low-AI industry groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option C: Simple industry-based AI exposure classification\n",
    "# This is a starting point - can be refined with Felten et al. data later\n",
    "\n",
    "# Industries with HIGH AI exposure (based on literature)\n",
    "HIGH_AI_EXPOSURE_INDUSTRIES = [\n",
    "    # Technology & Software\n",
    "    'software', 'technology', 'internet', 'it services', 'computer',\n",
    "    'semiconductor', 'electronic', 'telecommunications',\n",
    "    # Professional Services (high knowledge work)\n",
    "    'consulting', 'professional services', 'business services',\n",
    "    'advertising', 'marketing', 'media',\n",
    "    # Financial Services\n",
    "    'banking', 'financial services', 'insurance', 'asset management',\n",
    "    'investment', 'fintech',\n",
    "    # Healthcare (AI applications)\n",
    "    'healthcare', 'pharmaceuticals', 'biotech', 'medical devices',\n",
    "    # Customer Service Heavy\n",
    "    'retail', 'e-commerce', 'customer service'\n",
    "]\n",
    "\n",
    "# Industries with LOW AI exposure (based on literature)  \n",
    "LOW_AI_EXPOSURE_INDUSTRIES = [\n",
    "    # Physical/Manual work\n",
    "    'construction', 'mining', 'agriculture', 'forestry',\n",
    "    'utilities', 'oil', 'gas', 'energy',\n",
    "    # Manufacturing (physical)\n",
    "    'manufacturing', 'industrial', 'machinery', 'automotive',\n",
    "    # Transportation & Logistics\n",
    "    'transportation', 'logistics', 'shipping', 'trucking', 'airlines',\n",
    "    # Real Estate & Physical Assets\n",
    "    'real estate', 'reit', 'hospitality', 'hotels',\n",
    "    # Food & Beverage\n",
    "    'food', 'beverage', 'restaurant'\n",
    "]\n",
    "\n",
    "def classify_ai_exposure(industry_string):\n",
    "    \"\"\"\n",
    "    Classify industry into high/low AI exposure.\n",
    "    Returns: 1 (high), 0 (low), or np.nan (unclear)\n",
    "    \"\"\"\n",
    "    if pd.isna(industry_string):\n",
    "        return np.nan\n",
    "    \n",
    "    industry_lower = str(industry_string).lower()\n",
    "    \n",
    "    # Check high exposure\n",
    "    for keyword in HIGH_AI_EXPOSURE_INDUSTRIES:\n",
    "        if keyword in industry_lower:\n",
    "            return 1\n",
    "    \n",
    "    # Check low exposure\n",
    "    for keyword in LOW_AI_EXPOSURE_INDUSTRIES:\n",
    "        if keyword in industry_lower:\n",
    "            return 0\n",
    "    \n",
    "    # Unclear\n",
    "    return np.nan\n",
    "\n",
    "print(\"AI exposure classification function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classification if industry column exists\n",
    "if industry_col:\n",
    "    df['ai_exposure'] = df[industry_col].apply(classify_ai_exposure)\n",
    "    \n",
    "    print(\"AI Exposure Distribution:\")\n",
    "    print(df['ai_exposure'].value_counts(dropna=False))\n",
    "    print(f\"\\nClassified: {df['ai_exposure'].notna().sum()} / {len(df)} firms\")\n",
    "else:\n",
    "    print(\"Please identify the industry column from notebook 01 and update this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which industries couldn't be classified\n",
    "if industry_col and 'ai_exposure' in df.columns:\n",
    "    unclassified = df[df['ai_exposure'].isna()][industry_col].value_counts()\n",
    "    print(\"Unclassified Industries:\")\n",
    "    print(unclassified.head(20))\n",
    "    print(f\"\\nTotal unclassified: {len(unclassified)} unique industries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Felten et al. AI Exposure Scores (Advanced)\n",
    "\n",
    "For more rigorous measurement, use the Felten et al. (2023) scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Felten et al. AI exposure scores\n",
    "# Source: https://github.com/MITEcon/AI_Exposure or supplementary materials\n",
    "\n",
    "# This is a sample - actual data needs to be downloaded\n",
    "FELTEN_AI_SCORES_URL = \"https://raw.githubusercontent.com/MITEcon/AI_Exposure/main/data/ai_exposure_by_occupation.csv\"\n",
    "\n",
    "try:\n",
    "    felten_scores = pd.read_csv(FELTEN_AI_SCORES_URL)\n",
    "    print(f\"Felten AI scores loaded: {felten_scores.shape}\")\n",
    "    display(felten_scores.head())\n",
    "except Exception as e:\n",
    "    print(f\"Could not load Felten scores automatically: {e}\")\n",
    "    print(\"\\nPlease manually download from:\")\n",
    "    print(\"  - Felten et al. (2023): 'Occupational, industry, and geographic exposure to artificial intelligence'\")\n",
    "    print(\"  - NBER Working Paper or journal publication appendix\")\n",
    "    felten_scores = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Create industry-level AI exposure from O*NET + BLS data\n",
    "# This requires more setup but is more defensible academically\n",
    "\n",
    "def create_industry_ai_exposure():\n",
    "    \"\"\"\n",
    "    Create industry-level AI exposure scores.\n",
    "    \n",
    "    Steps:\n",
    "    1. Get occupation-level AI exposure (Felten et al.)\n",
    "    2. Get industry-occupation employment matrix (BLS OES)\n",
    "    3. Compute weighted average for each industry\n",
    "    \"\"\"\n",
    "    # This is a placeholder - actual implementation requires:\n",
    "    # 1. BLS Occupational Employment Statistics data\n",
    "    # 2. Industry-occupation crosswalk\n",
    "    \n",
    "    # For now, use academic consensus estimates\n",
    "    # Source: Multiple papers including Acemoglu & Restrepo, Webb (2020)\n",
    "    \n",
    "    industry_scores = {\n",
    "        # NAICS 2-digit codes with AI exposure scores (0-1 scale)\n",
    "        '51': 0.85,  # Information\n",
    "        '52': 0.75,  # Finance and Insurance\n",
    "        '54': 0.80,  # Professional, Scientific, Technical Services\n",
    "        '55': 0.70,  # Management of Companies\n",
    "        '56': 0.65,  # Administrative and Support Services\n",
    "        '62': 0.60,  # Health Care\n",
    "        '61': 0.55,  # Educational Services\n",
    "        '42': 0.50,  # Wholesale Trade\n",
    "        '44': 0.45,  # Retail Trade\n",
    "        '45': 0.45,  # Retail Trade\n",
    "        '31': 0.40,  # Manufacturing\n",
    "        '32': 0.40,  # Manufacturing\n",
    "        '33': 0.40,  # Manufacturing\n",
    "        '48': 0.35,  # Transportation\n",
    "        '49': 0.35,  # Warehousing\n",
    "        '72': 0.30,  # Accommodation and Food Services\n",
    "        '23': 0.25,  # Construction\n",
    "        '21': 0.20,  # Mining\n",
    "        '11': 0.15,  # Agriculture\n",
    "    }\n",
    "    \n",
    "    return industry_scores\n",
    "\n",
    "industry_ai_scores = create_industry_ai_exposure()\n",
    "print(\"Industry AI Exposure Scores (NAICS 2-digit):\")\n",
    "for naics, score in sorted(industry_ai_scores.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {naics}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge AI Exposure to Firm Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have SIC or NAICS codes\n",
    "sic_col = None\n",
    "naics_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'sic' in col_lower:\n",
    "        sic_col = col\n",
    "        print(f\"Found SIC column: {col}\")\n",
    "        print(f\"  Sample values: {df[col].dropna().head(5).tolist()}\")\n",
    "    if 'naics' in col_lower:\n",
    "        naics_col = col\n",
    "        print(f\"Found NAICS column: {col}\")\n",
    "        print(f\"  Sample values: {df[col].dropna().head(5).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map NAICS to AI exposure if available\n",
    "if naics_col:\n",
    "    def get_naics_ai_score(naics_code):\n",
    "        \"\"\"Get AI exposure score based on NAICS code.\"\"\"\n",
    "        if pd.isna(naics_code):\n",
    "            return np.nan\n",
    "        naics_str = str(int(naics_code))[:2] if not pd.isna(naics_code) else None\n",
    "        return industry_ai_scores.get(naics_str, np.nan)\n",
    "    \n",
    "    df['ai_exposure_continuous'] = df[naics_col].apply(get_naics_ai_score)\n",
    "    df['ai_exposure_binary'] = (df['ai_exposure_continuous'] > df['ai_exposure_continuous'].median()).astype(float)\n",
    "    \n",
    "    print(\"AI Exposure Score Distribution:\")\n",
    "    print(df['ai_exposure_continuous'].describe())\n",
    "else:\n",
    "    print(\"No NAICS column found. Using text-based classification from above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate Treatment Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AI exposure distribution\n",
    "exposure_col = 'ai_exposure_continuous' if 'ai_exposure_continuous' in df.columns else 'ai_exposure'\n",
    "\n",
    "if exposure_col in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Distribution\n",
    "    df[exposure_col].hist(bins=30, ax=axes[0], color='steelblue', alpha=0.7)\n",
    "    axes[0].set_xlabel('AI Exposure Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of AI Exposure')\n",
    "    axes[0].axvline(df[exposure_col].median(), color='red', linestyle='--', label='Median')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # By industry (if available)\n",
    "    if industry_col:\n",
    "        industry_exposure = df.groupby(industry_col)[exposure_col].mean().sort_values(ascending=False).head(15)\n",
    "        industry_exposure.plot(kind='barh', ax=axes[1], color='steelblue')\n",
    "        axes[1].set_xlabel('Mean AI Exposure')\n",
    "        axes[1].set_title('AI Exposure by Industry (Top 15)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check balance between treatment and control groups\n",
    "binary_exposure = 'ai_exposure_binary' if 'ai_exposure_binary' in df.columns else 'ai_exposure'\n",
    "\n",
    "if binary_exposure in df.columns:\n",
    "    # Find numeric columns for comparison\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns[:10]  # First 10 numeric\n",
    "    \n",
    "    print(\"Treatment vs Control Group Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col != binary_exposure:\n",
    "            high_exp = df[df[binary_exposure] == 1][col].mean()\n",
    "            low_exp = df[df[binary_exposure] == 0][col].mean()\n",
    "            diff = high_exp - low_exp\n",
    "            print(f\"{col[:40]:40} | High: {high_exp:12.2f} | Low: {low_exp:12.2f} | Diff: {diff:12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset with AI exposure measure\n",
    "output_file = DATA_PATH / 'data_with_ai_exposure.parquet'\n",
    "\n",
    "# Select key columns to save\n",
    "cols_to_save = list(df.columns)\n",
    "\n",
    "df[cols_to_save].to_parquet(output_file, index=False)\n",
    "print(f\"Saved processed data to: {output_file}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of treatment assignment\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TREATMENT ASSIGNMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'ai_exposure_binary' in df.columns:\n",
    "    print(f\"\\nBinary classification:\")\n",
    "    print(f\"  High AI exposure (treatment): {(df['ai_exposure_binary'] == 1).sum():,} firms\")\n",
    "    print(f\"  Low AI exposure (control): {(df['ai_exposure_binary'] == 0).sum():,} firms\")\n",
    "    print(f\"  Unclassified: {df['ai_exposure_binary'].isna().sum():,} firms\")\n",
    "\n",
    "if 'ai_exposure_continuous' in df.columns:\n",
    "    print(f\"\\nContinuous measure:\")\n",
    "    print(f\"  Mean: {df['ai_exposure_continuous'].mean():.3f}\")\n",
    "    print(f\"  Std: {df['ai_exposure_continuous'].std():.3f}\")\n",
    "    print(f\"  Min: {df['ai_exposure_continuous'].min():.3f}\")\n",
    "    print(f\"  Max: {df['ai_exposure_continuous'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Review classification**: Check if the industry mapping makes sense\n",
    "2. **Refine if needed**: Add more industry keywords or use NAICS mapping\n",
    "3. **Proceed to notebook 03**: Construct the panel dataset for DiD analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
